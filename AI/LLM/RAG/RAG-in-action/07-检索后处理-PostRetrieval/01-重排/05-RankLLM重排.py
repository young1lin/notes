from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
import torch

"""
RankLLMé‡æ’ç®—æ³•å®ç°

RankLLMæ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡æ’æ–¹æ³•ï¼Œåˆ©ç”¨LLMå¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›è¿›è¡Œæ–‡æ¡£é‡æ’ã€‚

æ ¸å¿ƒåŸç†ï¼š
1. åˆ©ç”¨LLMçš„æ·±åº¦è¯­è¨€ç†è§£èƒ½åŠ›åˆ¤æ–­æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§
2. é€šè¿‡prompt engineeringå¼•å¯¼LLMè¿›è¡Œæ’åºå†³ç­–
3. ç»“åˆLLMçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è¯­ä¹‰å…³ç³»

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- è¯­ä¹‰ç†è§£æ·±åº¦ï¼šåŸºäºLLMçš„å¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›
- æ¨ç†èƒ½åŠ›å¼ºï¼šèƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„é€»è¾‘æ¨ç†å’Œè¯­ä¹‰åŒ¹é…
- çµæ´»æ€§é«˜ï¼šå¯ä»¥é€šè¿‡promptè°ƒæ•´é€‚åº”ä¸åŒé¢†åŸŸå’Œä»»åŠ¡
- è§£é‡Šæ€§å¥½ï¼šLLMå¯ä»¥æä¾›æ’åºçš„ç†ç”±å’Œè§£é‡Š

ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”ï¼š
- vs BERTç±»æ¨¡å‹ï¼šè¯­ä¹‰ç†è§£æ›´æ·±å…¥ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æ¨ç†
- vs ä¼ ç»Ÿé‡æ’ï¼šèƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å’Œéšå«ä¿¡æ¯
- vs åµŒå…¥æ¨¡å‹ï¼šä¸ä»…è€ƒè™‘ç›¸ä¼¼åº¦ï¼Œè¿˜è€ƒè™‘é€»è¾‘å…³ç³»

é€‚ç”¨åœºæ™¯ï¼š
- å¯¹ç²¾åº¦è¦æ±‚æé«˜çš„åº”ç”¨
- éœ€è¦å¤æ‚æ¨ç†çš„æŸ¥è¯¢
- é¢†åŸŸä¸“ä¸šæ€§å¼ºçš„æ–‡æ¡£æ£€ç´¢
- éœ€è¦å¯è§£é‡Šæ€§çš„é‡æ’ä»»åŠ¡

æ³¨æ„äº‹é¡¹ï¼š
- è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼ˆè°ƒç”¨LLM APIï¼‰
- å»¶è¿Ÿç›¸å¯¹è¾ƒå¤§
- éœ€è¦åˆç†è®¾è®¡prompt
"""

print("ğŸ”„ åˆå§‹åŒ–RankLLMé‡æ’ç³»ç»Ÿ...")

# 1. æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†
print("ğŸ“– åŠ è½½å’Œé¢„å¤„ç†æ–‡æ¡£...")
doc_path = "90-æ–‡æ¡£-Data/å±±è¥¿æ–‡æ—…/äº‘å†ˆçŸ³çªŸ.txt"
print(f"æ–‡æ¡£è·¯å¾„: {doc_path}")

print("  ğŸ”¤ ä½¿ç”¨TextLoaderåŠ è½½æ–‡æ¡£...")
documents = TextLoader(doc_path).load()
print(f"  âœ… æˆåŠŸåŠ è½½æ–‡æ¡£ï¼ŒåŸå§‹æ–‡æ¡£æ•°é‡: {len(documents)}")

print("  âœ‚ï¸  å¼€å§‹æ–‡æ¡£åˆ†å‰²...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,       # æ¯ä¸ªæ–‡æ¡£å—500ä¸ªå­—ç¬¦
    chunk_overlap=100     # å—ä¹‹é—´é‡å 100ä¸ªå­—ç¬¦ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
)
texts = text_splitter.split_documents(documents)
print(f"  ğŸ“Š åˆ†å‰²åæ–‡æ¡£å—æ•°é‡: {len(texts)}")

# ä¸ºæ¯ä¸ªæ–‡æ¡£å—æ·»åŠ å”¯ä¸€ID
print("  ğŸ†” ä¸ºæ–‡æ¡£å—æ·»åŠ å”¯ä¸€æ ‡è¯†...")
for idx, text in enumerate(texts):
    text.metadata["id"] = idx
    text.metadata["chunk_size"] = len(text.page_content)
print("  âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆ")

# 2. åˆ›å»ºå‘é‡æ£€ç´¢å™¨
print(f"\nğŸ” åˆ›å»ºFAISSå‘é‡æ£€ç´¢å™¨...")
print("  ğŸ“¥ åŠ è½½ä¸­æ–‡åµŒå…¥æ¨¡å‹...")
embed_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh")  # ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„åµŒå…¥æ¨¡å‹
print("  ğŸ—ï¸  æ„å»ºFAISSå‘é‡ç´¢å¼•...")
retriever = FAISS.from_documents(texts, embed_model).as_retriever(
    search_kwargs={"k": 20}  # ç¬¬ä¸€é˜¶æ®µæ£€ç´¢Top-20æ–‡æ¡£
)
print(f"  âœ… å‘é‡æ£€ç´¢å™¨åˆ›å»ºå®Œæˆï¼Œå°†è¿”å›Top-20å€™é€‰æ–‡æ¡£")

# 3. GPUå†…å­˜ä¼˜åŒ–ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
print(f"\nğŸ§¹ ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨...")
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("  ğŸ—‘ï¸  å·²æ¸…ç†GPUç¼“å­˜")
else:
    print("  ğŸ’» å½“å‰ä½¿ç”¨CPUæ¨¡å¼")

# 4. é…ç½®RankLLMé‡æ’å™¨
print(f"\nğŸ¤– é…ç½®RankLLMé‡æ’å™¨...")
print("  âš™ï¸  RankLLMé…ç½®å‚æ•°:")
print("    - top_n: 3 (æœ€ç»ˆè¿”å›å‰3ä¸ªæ–‡æ¡£)")
print("    - model: gpt (ä½¿ç”¨GPTæ¨¡å‹)")
print("    - gpt_model: gpt-4o-mini (é«˜æ•ˆçš„GPTæ¨¡å‹)")

# é…ç½®OPENAI ä»£ç†ä¿¡æ¯
# OPENAI_BASE_URL = "https://vip.apiyi.com/v1"
# OPENAI_API_KEY = ""
# os.environ["OPENAI_BASE_URL"] = OPENAI_BASE_URL
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

compressor = RankLLMRerank(
    top_n=3,                    # æœ€ç»ˆè¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
    model="gpt",                # ä½¿ç”¨GPTæ¨¡å‹è¿›è¡Œé‡æ’
    gpt_model="gpt-4o-mini"     # é€‰æ‹©é«˜æ•ˆçš„GPT-4o-miniæ¨¡å‹
)
print("  âœ… RankLLMé‡æ’å™¨é…ç½®å®Œæˆ")

# 5. åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨
print(f"\nğŸ”— åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨...")
print("  ğŸ“‹ ç»„åˆå‘é‡æ£€ç´¢å™¨å’ŒRankLLMé‡æ’å™¨...")
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,     # ä½¿ç”¨RankLLMä½œä¸ºå‹ç¼©å™¨ï¼ˆé‡æ’å™¨ï¼‰
    base_retriever=retriever        # ä½¿ç”¨FAISSä½œä¸ºåŸºç¡€æ£€ç´¢å™¨
)
print("  âœ… æ£€ç´¢é“¾æ¡æ„å»ºå®Œæˆï¼šFAISSæ£€ç´¢ â†’ RankLLMé‡æ’")

# 6. æ‰§è¡ŒæŸ¥è¯¢å’Œé‡æ’
print(f"\nğŸ¯ å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢å’Œé‡æ’...")
query = "äº‘å†ˆçŸ³çªŸæœ‰å“ªäº›è‘—åçš„é€ åƒï¼Ÿ"
print(f"æŸ¥è¯¢é—®é¢˜: {query}")

print(f"\nç¬¬ä¸€é˜¶æ®µ - FAISSå‘é‡æ£€ç´¢:")
print("  ğŸ” åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢å€™é€‰æ–‡æ¡£...")

print(f"\nç¬¬äºŒé˜¶æ®µ - RankLLMé‡æ’:")
print("  ğŸ¤– è°ƒç”¨GPTæ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰é‡æ’...")
print("  â³ æ­£åœ¨å¤„ç†ä¸­ï¼ˆLLMæ¨ç†éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")

try:
    compressed_docs = compression_retriever.invoke(query)
    print(f"  âœ… RankLLMé‡æ’å®Œæˆ")
    print(f"  ğŸ“Š æœ€ç»ˆè¿”å› {len(compressed_docs)} ä¸ªé«˜è´¨é‡æ–‡æ¡£")

    # 7. æ ¼å¼åŒ–è¾“å‡ºé‡æ’ç»“æœ
    def pretty_print_docs(docs):
        """
        ç¾åŒ–æ–‡æ¡£è¾“å‡ºå‡½æ•°
        
        åŠŸèƒ½ï¼šä»¥æ˜“è¯»çš„æ ¼å¼å±•ç¤ºé‡æ’åçš„æ–‡æ¡£ç»“æœ
        
        å‚æ•°ï¼š
            docs (list): é‡æ’åçš„æ–‡æ¡£åˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"ğŸ† RankLLMé‡æ’æœ€ç»ˆç»“æœ")
        print(f"{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"é‡æ’åæ–‡æ¡£ï¼ˆæŒ‰ç›¸å…³æ€§é™åºï¼‰:")
        
        result_parts = []
        for i, doc in enumerate(docs, 1):
            doc_info = f"\nğŸ“„ æ’å {i}:\n"
            doc_info += f"   æ–‡æ¡£å†…å®¹:\n{doc.page_content}\n"
            
            # æ˜¾ç¤ºæ–‡æ¡£å…ƒæ•°æ®
            if hasattr(doc, 'metadata') and doc.metadata:
                doc_info += f"   æ–‡æ¡£ID: {doc.metadata.get('id', 'æœªçŸ¥')}\n"
                doc_info += f"   å†…å®¹é•¿åº¦: {doc.metadata.get('chunk_size', len(doc.page_content))} å­—ç¬¦\n"
                if 'source' in doc.metadata:
                    doc_info += f"   æ¥æºæ–‡ä»¶: {doc.metadata['source']}\n"
            
            result_parts.append(doc_info)
        
        return "\n" + ("-" * 100) + "\n".join(result_parts)

    # è¾“å‡ºç¾åŒ–çš„ç»“æœ
    formatted_result = pretty_print_docs(compressed_docs)
    print(formatted_result)

except Exception as e:
    print(f"  âŒ RankLLMé‡æ’å¤±è´¥: {str(e)}")
    print("  ğŸ’¡ å¯èƒ½çš„åŸå› :")
    print("    - GPT APIå¯†é’¥æœªé…ç½®æˆ–æ— æ•ˆ")
    print("    - ç½‘ç»œè¿æ¥é—®é¢˜")
    print("    - APIé…é¢å·²ç”¨å®Œ")
    print("    - æ–‡æ¡£å†…å®¹æ ¼å¼é—®é¢˜")
    print("  ğŸ”§ å»ºè®®æ£€æŸ¥:")
    print("    - OpenAI APIå¯†é’¥é…ç½®")
    print("    - ç½‘ç»œè¿æ¥çŠ¶æ€")
    print("    - æ–‡æ¡£æ–‡ä»¶æ˜¯å¦å­˜åœ¨")

# 8. èµ„æºæ¸…ç†
print(f"\nğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº...")
try:
    # æ¸…ç†RankLLMæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if 'compressor' in locals():
        del compressor
        print("  ğŸ—‘ï¸  å·²é‡Šæ”¾RankLLMæ¨¡å‹èµ„æº")
    
    # å†æ¬¡æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("  ğŸ—‘ï¸  å·²æ¸…ç†GPUç¼“å­˜")
    
    print("  âœ… èµ„æºæ¸…ç†å®Œæˆ")
except Exception as e:
    print(f"  âš ï¸  èµ„æºæ¸…ç†æ—¶å‡ºç°è­¦å‘Š: {str(e)}")

print(f"\nğŸ“‹ RankLLMé‡æ’æ€»ç»“:")
print("- âœ… æ·±åº¦ç†è§£ï¼šåŸºäºLLMçš„å¼ºå¤§è¯­è¨€ç†è§£èƒ½åŠ›")
print("- âœ… æ¨ç†èƒ½åŠ›ï¼šèƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„é€»è¾‘æ¨ç†å’Œè¯­ä¹‰åŒ¹é…")
print("- âœ… é«˜ç²¾åº¦ï¼šåˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹æŠ€æœ¯")
print("- âœ… å¯è§£é‡Šï¼šLLMå¯ä»¥æä¾›æ’åºçš„ç†ç”±å’Œä¾æ®")
print("- âš ï¸  é«˜æˆæœ¬ï¼šéœ€è¦è°ƒç”¨LLM APIï¼Œæˆæœ¬ç›¸å¯¹è¾ƒé«˜")
print("- âš ï¸  é«˜å»¶è¿Ÿï¼šLLMæ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒé•¿")
print("- ğŸ’¡ æœ€ä½³å®è·µï¼šé€‚ç”¨äºå¯¹ç²¾åº¦è¦æ±‚æé«˜çš„é‡è¦æŸ¥è¯¢")
print("- ğŸ”§ ä¼˜åŒ–å»ºè®®ï¼šåˆç†è®¾è®¡promptä»¥æå‡é‡æ’æ•ˆæœ")
