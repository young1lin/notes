#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä¸Šä¸‹æ–‡æ£€ç´¢ï¼ˆContextual Retrievalï¼‰ä¸Milvuså®ç°
åŸºäºAnthropicæå‡ºçš„æ–¹æ³•ï¼Œè§£å†³ä¼ ç»ŸRAGä¸­è¯­ä¹‰éš”ç¦»é—®é¢˜

æ ¸å¿ƒæ¦‚å¿µï¼š
1. ä¼ ç»ŸRAGé—®é¢˜ï¼šæ–‡æ¡£è¢«åˆ†å‰²æˆç‹¬ç«‹çš„å—ï¼Œä¸¢å¤±äº†ä¸Šä¸‹æ–‡ä¿¡æ¯
2. ä¸Šä¸‹æ–‡æ£€ç´¢è§£å†³æ–¹æ¡ˆï¼šä¸ºæ¯ä¸ªå—æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶è¯­ä¹‰æ›´å®Œæ•´
3. æ·±åº¦è¯„ä¼°ï¼ˆDeep Evaluationï¼‰ï¼šå¤šç»´åº¦è¯„ä¼°æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½

æŠ€æœ¯æ ˆï¼š
- Milvus: å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒå¯†é›†å‘é‡å’Œç¨€ç–å‘é‡
- SentenceTransformer: ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤º
- OpenAI GPT: ç”¨äºç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„æ–‡æœ¬å—ï¼ˆæ›¿ä»£Claudeï¼‰
- Cohere Reranker: é‡æ’åºæ¨¡å‹ï¼Œä¼˜åŒ–æ£€ç´¢ç»“æœ

APIå˜æ›´è¯´æ˜ï¼š
- åŸç‰ˆæœ¬ä½¿ç”¨Anthropic Claude APIè¿›è¡Œä¸Šä¸‹æ–‡å¢å¼º
- å½“å‰ç‰ˆæœ¬æ”¹ç”¨OpenAI GPT APIï¼Œæä¾›æ›´å¥½çš„å¯ç”¨æ€§å’Œæ€§èƒ½
- Claudeç›¸å…³ä»£ç å·²æ³¨é‡Šï¼Œå¯æ ¹æ®éœ€è¦åˆ‡æ¢å›æ¥

===============================================================================
ğŸ“‹ ä»£ç ç»“æ„åˆ†æ (Code Structure Analysis)
===============================================================================

ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAGä¸Šä¸‹æ–‡æ£€ç´¢ç³»ç»Ÿæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å…¥å±‚: åŸå§‹æ–‡æ¡£ â†’ æ–‡æ¡£åˆ†å— â†’ ä¸Šä¸‹æ–‡å¢å¼º â†’ å‘é‡åŒ– â†’ å­˜å‚¨           â”‚
â”‚  æ£€ç´¢å±‚: æŸ¥è¯¢å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æœç´¢ â†’ é‡æ’åº â†’ ç»“æœè¿”å›              â”‚
â”‚  è¯„ä¼°å±‚: é»„é‡‘æ ‡å‡†å¯¹æ¯” â†’ æ€§èƒ½æŒ‡æ ‡è®¡ç®— â†’ ç»“æœåˆ†æ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”§ æ¨¡å—èŒè´£åˆ’åˆ†:
1. MilvusContextualRetriever (æ ¸å¿ƒæ£€ç´¢å™¨)
   - è´Ÿè´£å‘é‡æ•°æ®åº“çš„æ“ä½œ
   - å®ç°å¤šç§æ£€ç´¢ç­–ç•¥
   - ç®¡ç†ä¸Šä¸‹æ–‡åŒ–å’Œé‡æ’åºæµç¨‹

2. è¯„ä¼°æ¨¡å— (Performance Evaluation)
   - evaluate_retrieval(): æ ¸å¿ƒè¯„ä¼°é€»è¾‘
   - evaluate_db(): æ•°æ®åº“æ€§èƒ½è¯„ä¼°
   - retrieve_base(): åŸºç¡€æ£€ç´¢æ¥å£

3. æ•°æ®å¤„ç†æ¨¡å— (Data Processing)
   - download_data(): æ•°æ®ä¸‹è½½
   - load_jsonl(): æ•°æ®åŠ è½½
   - æ•°æ®æ ¼å¼æ ‡å‡†åŒ–

4. å®éªŒæ§åˆ¶æ¨¡å— (Experiment Control)
   - main(): å®éªŒæµç¨‹æ§åˆ¶
   - ä¸‰ç§æ£€ç´¢ç­–ç•¥å¯¹æ¯”
   - æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡

===============================================================================
ğŸ”„ æ•°æ®æµç¨‹åˆ†æ (Data Flow Analysis)
===============================================================================

ğŸ“Š æ•°æ®å¤„ç†æµç¨‹:
åŸå§‹æ–‡æ¡£ â†’ æ–‡æ¡£åˆ†å— â†’ [å¯é€‰]ä¸Šä¸‹æ–‡å¢å¼º â†’ å‘é‡åŒ– â†’ Milvuså­˜å‚¨
    â†“
æŸ¥è¯¢è¾“å…¥ â†’ æŸ¥è¯¢å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æœç´¢ â†’ [é‡æ’åº] â†’ ç»“æœè¾“å‡º
    â†“
è¯„ä¼°å¯¹æ¯” â†’ æ€§èƒ½æŒ‡æ ‡ â†’ ç»“æœåˆ†æ

ğŸ¯ æ£€ç´¢ç­–ç•¥å¯¹æ¯”:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç­–ç•¥ç±»å‹    â”‚   æ•°æ®é¢„å¤„ç†  â”‚   æ£€ç´¢æ–¹æ³•    â”‚   åå¤„ç†     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ ‡å‡†æ£€ç´¢     â”‚ åŸå§‹æ–‡æœ¬å—   â”‚ å¯†é›†å‘é‡æœç´¢ â”‚ æ—            â”‚
â”‚ ä¸Šä¸‹æ–‡æ£€ç´¢   â”‚ LLMå¢å¼ºå—    â”‚ å¯†é›†å‘é‡æœç´¢ â”‚ æ—            â”‚
â”‚ é‡æ’åºæ£€ç´¢   â”‚ LLMå¢å¼ºå—    â”‚ å¯†é›†å‘é‡æœç´¢ â”‚ Cohereé‡æ’åº â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” è¯„ä¼°ä½“ç³»:
è¾“å…¥: æŸ¥è¯¢ + é»„é‡‘æ ‡å‡†ç­”æ¡ˆ
å¤„ç†: æ£€ç´¢ â†’ åŒ¹é… â†’ è®¡åˆ†
è¾“å‡º: Pass@Kåˆ†æ•°ã€å¹³å‡åˆ†æ•°ã€å¬å›ç‡

===============================================================================
âš¡ æ‰§è¡Œæµç¨‹åˆ†æ (Execution Flow Analysis)
===============================================================================

ğŸš€ ä¸»è¦æ‰§è¡Œæ­¥éª¤:
1ï¸âƒ£ ç¯å¢ƒåˆå§‹åŒ–
   - åŠ è½½APIå¯†é’¥å’Œé…ç½®
   - åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’Œé‡æ’åºæ¨¡å‹
   - ä¸‹è½½ç¤ºä¾‹æ•°æ®

2ï¸âƒ£ æ•°æ®å‡†å¤‡
   - åŠ è½½æ–‡æ¡£æ•°æ®é›†
   - åˆ›å»ºè¯„ä¼°æŸ¥è¯¢é›†
   - æ•°æ®æ ¼å¼éªŒè¯

3ï¸âƒ£ å®éªŒä¸€: æ ‡å‡†æ£€ç´¢åŸºçº¿
   - åˆ›å»ºæ ‡å‡†Milvusé›†åˆ
   - æ’å…¥åŸå§‹æ–‡æœ¬å—
   - æ‰§è¡Œæ£€ç´¢è¯„ä¼°

4ï¸âƒ£ å®éªŒäºŒ: ä¸Šä¸‹æ–‡æ£€ç´¢
   - åˆ›å»ºä¸Šä¸‹æ–‡Milvusé›†åˆ
   - LLMå¢å¼ºæ–‡æœ¬å—
   - æ’å…¥å¢å¼ºåçš„æ–‡æœ¬å—
   - æ‰§è¡Œæ£€ç´¢è¯„ä¼°

5ï¸âƒ£ å®éªŒä¸‰: é‡æ’åºæ£€ç´¢
   - ä½¿ç”¨ä¸Šä¸‹æ–‡æ£€ç´¢å™¨
   - å¯ç”¨Cohereé‡æ’åº
   - æ‰§è¡Œæ£€ç´¢è¯„ä¼°

6ï¸âƒ£ ç»“æœåˆ†æ
   - å¯¹æ¯”ä¸‰ç§ç­–ç•¥æ€§èƒ½
   - è®¡ç®—æ€§èƒ½æå‡å¹…åº¦
   - ç”Ÿæˆåˆ†ææŠ¥å‘Š

===============================================================================
ğŸ›ï¸ å…³é”®å‚æ•°é…ç½® (Key Parameters)
===============================================================================

ğŸ“‹ å‘é‡æ•°æ®åº“é…ç½®:
- é›†åˆåç§°: åŒºåˆ†ä¸åŒå®éªŒçš„æ•°æ®é›†åˆ
- å‘é‡ç»´åº¦: ç”±åµŒå…¥æ¨¡å‹å†³å®šï¼ˆå¦‚BGE-large-zhä¸º1024ç»´ï¼‰
- ç´¢å¼•ç±»å‹: FLATï¼ˆç²¾ç¡®æœç´¢ï¼‰+ SPARSE_INVERTED_INDEXï¼ˆç¨€ç–å‘é‡ï¼‰
- è·ç¦»åº¦é‡: å†…ç§¯ï¼ˆIPï¼‰

ğŸ¤– LLMé…ç½®:
- æ¨¡å‹: gpt-3.5-turboï¼ˆå¿«é€Ÿç»æµå‹ï¼‰æˆ–gpt-4ï¼ˆé«˜è´¨é‡ï¼‰
- æœ€å¤§tokens: 1000
- æ¸©åº¦: 0ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
- API: OpenAI ChatGPT API

ğŸ”„ æ£€ç´¢é…ç½®:
- æ£€ç´¢æ•°é‡K: é»˜è®¤5ï¼ˆPass@5è¯„ä¼°ï¼‰
- æœç´¢å‚æ•°: nprobe=10
- é‡æ’åº: Cohere Rerank API

===============================================================================
"""

# å¯¼å…¥å¿…è¦çš„åº“
from pymilvus.model.dense import SentenceTransformerEmbeddingFunction
from pymilvus.model.hybrid import BGEM3EmbeddingFunction
from pymilvus.model.reranker import CohereRerankFunction

from typing import List, Dict, Any
from typing import Callable
from pymilvus import (
    MilvusClient,
    DataType,
    AnnSearchRequest,
    RRFRanker,
)
from tqdm import tqdm
import json
# import anthropic  # æ³¨é‡Šæ‰Claude APIï¼Œæ”¹ç”¨OpenAI
import openai  # æ–°å¢OpenAI APIæ”¯æŒ
import os
import dotenv
dotenv.load_dotenv()

class MilvusContextualRetriever:
    """
    Milvusä¸Šä¸‹æ–‡æ£€ç´¢å™¨ç±»
    
    ğŸ›ï¸ æ¶æ„è®¾è®¡:
    è¿™ä¸ªç±»æ˜¯æ•´ä¸ªæ£€ç´¢ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥çš„çµæ´»ç»„åˆã€‚
    
    ğŸ“¦ åŠŸèƒ½æ¨¡å—:
    1. æ ‡å‡†æ£€ç´¢ï¼šåŸºäºåŸå§‹æ–‡æœ¬å—çš„å‘é‡æ£€ç´¢
    2. æ··åˆæ£€ç´¢ï¼šç»“åˆå¯†é›†å‘é‡å’Œç¨€ç–å‘é‡çš„æ£€ç´¢
    3. ä¸Šä¸‹æ–‡æ£€ç´¢ï¼šä½¿ç”¨LLMä¸°å¯Œæ–‡æœ¬å—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åè¿›è¡Œæ£€ç´¢
    4. é‡æ’åºæ£€ç´¢ï¼šåœ¨æ£€ç´¢ç»“æœåŸºç¡€ä¸Šä½¿ç”¨ä¸“é—¨çš„é‡æ’åºæ¨¡å‹ä¼˜åŒ–ç»“æœ
    
    ğŸ”„ æ•°æ®æµå‘:
    æ–‡æœ¬è¾“å…¥ â†’ [ä¸Šä¸‹æ–‡å¢å¼º] â†’ å‘é‡åŒ– â†’ Milvuså­˜å‚¨
           â†“
    æŸ¥è¯¢è¾“å…¥ â†’ å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æœç´¢ â†’ [é‡æ’åº] â†’ ç»“æœè¾“å‡º
    
    ğŸ¯ è®¾è®¡åŸåˆ™:
    - å•ä¸€èŒè´£ï¼šæ¯ä¸ªæ–¹æ³•è´Ÿè´£ä¸€ä¸ªç‰¹å®šåŠŸèƒ½
    - å¼€æ”¾å°é—­ï¼šæ”¯æŒæ‰©å±•æ–°çš„æ£€ç´¢ç­–ç•¥
    - ä¾èµ–æ³¨å…¥ï¼šé€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥ä¾èµ–ç»„ä»¶
    - é…ç½®é©±åŠ¨ï¼šé€šè¿‡å‚æ•°æ§åˆ¶ä¸åŒåŠŸèƒ½çš„å¯ç”¨
    
    æ”¯æŒæ ‡å‡†æ£€ç´¢ã€æ··åˆæ£€ç´¢ã€ä¸Šä¸‹æ–‡æ£€ç´¢å’Œé‡æ’åºåŠŸèƒ½
    """
    
    def __init__(
        self,
        uri="milvus.db",
        collection_name="contexual_bgem3",
        dense_embedding_function=None,
        use_sparse=False,
        sparse_embedding_function=None,
        use_contextualize_embedding=False,
        llm_client=None,  # æ”¹ç”¨é€šç”¨LLMå®¢æˆ·ç«¯åç§°ï¼ˆæ”¯æŒOpenAIï¼‰
        use_reranker=False,
        rerank_function=None,
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        ğŸ”§ åˆå§‹åŒ–æµç¨‹:
        1. è®¾ç½®Milvusè¿æ¥å‚æ•°
        2. é…ç½®åµŒå…¥å‡½æ•°ï¼ˆå¯†é›†+ç¨€ç–ï¼‰
        3. è®¾ç½®LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºä¸Šä¸‹æ–‡å¢å¼ºï¼‰
        4. é…ç½®é‡æ’åºåŠŸèƒ½
        5. å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†
        
        å‚æ•°è¯´æ˜ï¼š
            uri: MilvusæœåŠ¡åœ°å€
                - æœ¬åœ°æ–‡ä»¶æ¨¡å¼ï¼šå¦‚ "./milvus.db"ï¼ˆMilvus Liteï¼‰
                - æœåŠ¡å™¨æ¨¡å¼ï¼šå¦‚ "http://localhost:19530"ï¼ˆç‹¬ç«‹MilvusæœåŠ¡ï¼‰
                - äº‘æœåŠ¡æ¨¡å¼ï¼šZilliz Cloudçš„è¿æ¥åœ°å€
            collection_name: é›†åˆåç§°ï¼Œç±»ä¼¼äºæ•°æ®åº“ä¸­çš„è¡¨å
            dense_embedding_function: å¯†é›†å‘é‡åµŒå…¥å‡½æ•°
                - ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´ç¨ å¯†å‘é‡ï¼ˆå¦‚768ç»´ã€1024ç»´ç­‰ï¼‰
                - é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¦‚BGEã€SentenceTransformerç­‰
            use_sparse: æ˜¯å¦ä½¿ç”¨ç¨€ç–å‘é‡
                - ç¨€ç–å‘é‡ç±»ä¼¼äºTF-IDFï¼Œä¸»è¦æ•è·å…³é”®è¯åŒ¹é…ä¿¡æ¯
                - ä¸å¯†é›†å‘é‡ç»“åˆå¯ä»¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§
            sparse_embedding_function: ç¨€ç–å‘é‡åµŒå…¥å‡½æ•°
            use_contextualize_embedding: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡åµŒå…¥
                - è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½ï¼šä½¿ç”¨LLMä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡
                - è§£å†³ä¼ ç»ŸRAGä¸­æ–‡æœ¬å—ç¼ºä¹ä¸Šä¸‹æ–‡çš„é—®é¢˜
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆæ”¯æŒOpenAI GPTæˆ–Claudeï¼‰
                - ç”¨äºç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„æ–‡æœ¬å—
                - å½“å‰ç‰ˆæœ¬ä¸»è¦æ”¯æŒOpenAI GPT-3.5/GPT-4
                - åŸClaude APIä»£ç å·²æ³¨é‡Šä¿ç•™
            use_reranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº
                - åœ¨åˆæ­¥æ£€ç´¢ç»“æœåŸºç¡€ä¸Šï¼Œä½¿ç”¨ä¸“é—¨çš„é‡æ’åºæ¨¡å‹ä¼˜åŒ–ç»“æœæ’åº
            rerank_function: é‡æ’åºå‡½æ•°ï¼ˆå¦‚Cohere Rerankï¼‰
        """
        self.collection_name = collection_name

        # å¯¹äºMilvus-liteï¼Œuriæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå¦‚"./milvus.db"
        # å¯¹äºMilvusç‹¬ç«‹æœåŠ¡ï¼Œuriç±»ä¼¼"http://localhost:19530"
        # å¯¹äºZilliz Cloudï¼Œè¯·è®¾ç½®`uri`å’Œ`token`
        self.client = MilvusClient(uri)

        self.embedding_function = dense_embedding_function

        self.use_sparse = use_sparse
        self.sparse_embedding_function = None

        self.use_contextualize_embedding = use_contextualize_embedding
        # self.anthropic_client = anthropic_client  # æ³¨é‡Šæ‰Claudeå®¢æˆ·ç«¯
        self.llm_client = llm_client  # æ”¹ç”¨é€šç”¨LLMå®¢æˆ·ç«¯

        self.use_reranker = use_reranker
        self.rerank_function = rerank_function

        # å‚æ•°éªŒè¯ï¼šå¦‚æœå¯ç”¨ç¨€ç–å‘é‡ï¼Œå¿…é¡»æä¾›ç¨€ç–åµŒå…¥å‡½æ•°
        if use_sparse is True and sparse_embedding_function:
            self.sparse_embedding_function = sparse_embedding_function
        elif use_sparse is True and sparse_embedding_function is None:
            raise ValueError(
                "ç¨€ç–åµŒå…¥å‡½æ•°ä¸èƒ½ä¸ºç©ºï¼Œå¦‚æœuse_sparseä¸ºTrue"
            )
        else:
            pass

    def build_collection(self):
        """
        æ„å»ºMilvusé›†åˆ
        
        ğŸ—ï¸ é›†åˆè®¾è®¡åŸç†:
        1. Schemaè®¾è®¡ï¼šå®šä¹‰æ•°æ®ç»“æ„å’Œå­—æ®µç±»å‹
        2. ç´¢å¼•ç­–ç•¥ï¼šé€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹ä»¥ä¼˜åŒ–æœç´¢æ€§èƒ½
        3. åŠ¨æ€å­—æ®µï¼šæ”¯æŒçµæ´»çš„å…ƒæ•°æ®å­˜å‚¨
        4. å‘é‡å­—æ®µï¼šæ”¯æŒå¯†é›†å’Œç¨€ç–å‘é‡çš„æ··åˆå­˜å‚¨
        
        ğŸ“Š å­˜å‚¨ç»“æ„:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    å­—æ®µ     â”‚    ç±»å‹      â”‚      ç”¨é€”       â”‚    ç´¢å¼•ç±»å‹   â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ pk          â”‚ INT64        â”‚ ä¸»é”®ID         â”‚ è‡ªåŠ¨          â”‚
        â”‚ dense_vectorâ”‚ FLOAT_VECTOR â”‚ è¯­ä¹‰å‘é‡       â”‚ FLAT/IP      â”‚
        â”‚ sparse_vectorâ”‚ SPARSE_VECTORâ”‚ å…³é”®è¯å‘é‡     â”‚ INVERTED/IP  â”‚
        â”‚ content     â”‚ VARCHAR      â”‚ åŸå§‹å†…å®¹       â”‚ åŠ¨æ€å­—æ®µ     â”‚
        â”‚ metadata    â”‚ JSON         â”‚ å…ƒæ•°æ®ä¿¡æ¯     â”‚ åŠ¨æ€å­—æ®µ     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        é›†åˆè®¾è®¡è¯´æ˜ï¼š
        1. ä½¿ç”¨åŠ¨æ€Schemaï¼Œæ”¯æŒçµæ´»çš„å­—æ®µæ·»åŠ 
        2. ä¸»é”®è‡ªåŠ¨ç”Ÿæˆï¼Œç¡®ä¿æ¯æ¡è®°å½•çš„å”¯ä¸€æ€§
        3. å¯†é›†å‘é‡å­—æ®µï¼šå­˜å‚¨æ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤º
        4. ç¨€ç–å‘é‡å­—æ®µï¼ˆå¯é€‰ï¼‰ï¼šå­˜å‚¨å…³é”®è¯ç›¸å…³çš„ç¨€ç–å‘é‡
        5. ç´¢å¼•ç­–ç•¥ï¼šå¯†é›†å‘é‡ä½¿ç”¨FLATç´¢å¼•ï¼Œç¨€ç–å‘é‡ä½¿ç”¨å€’æ’ç´¢å¼•
        """
        # åˆ›å»ºé›†åˆçš„Schemaå®šä¹‰
        schema = self.client.create_schema(
            auto_id=True,  # è‡ªåŠ¨ç”Ÿæˆä¸»é”®ID
            enable_dynamic_field=True,  # å…è®¸åŠ¨æ€æ·»åŠ å­—æ®µï¼Œæé«˜çµæ´»æ€§
        )
        
        # æ·»åŠ ä¸»é”®å­—æ®µ
        schema.add_field(field_name="pk", datatype=DataType.INT64, is_primary=True)
        
        # æ·»åŠ å¯†é›†å‘é‡å­—æ®µ - å­˜å‚¨æ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤º
        schema.add_field(
            field_name="dense_vector",
            datatype=DataType.FLOAT_VECTOR,
            dim=self.embedding_function.dim,  # å‘é‡ç»´åº¦ç”±åµŒå…¥å‡½æ•°å†³å®š
        )
        
        # å¦‚æœå¯ç”¨ç¨€ç–å‘é‡ï¼Œæ·»åŠ ç¨€ç–å‘é‡å­—æ®µ
        if self.use_sparse is True:
            schema.add_field(
                field_name="sparse_vector", datatype=DataType.SPARSE_FLOAT_VECTOR
            )

        # å‡†å¤‡ç´¢å¼•å‚æ•° - ç´¢å¼•ç”¨äºåŠ é€Ÿå‘é‡æœç´¢
        index_params = self.client.prepare_index_params()
        
        # ä¸ºå¯†é›†å‘é‡æ·»åŠ ç´¢å¼•
        # FLATç´¢å¼•ï¼šç²¾ç¡®æœç´¢ï¼Œé€‚åˆå°åˆ°ä¸­ç­‰è§„æ¨¡æ•°æ®é›†
        # IPï¼ˆå†…ç§¯ï¼‰è·ç¦»ï¼šé€‚åˆå½’ä¸€åŒ–åçš„å‘é‡
        index_params.add_index(
            field_name="dense_vector", index_type="FLAT", metric_type="IP"
        )
        
        # ä¸ºç¨€ç–å‘é‡æ·»åŠ å€’æ’ç´¢å¼•
        if self.use_sparse is True:
            index_params.add_index(
                field_name="sparse_vector",
                index_type="SPARSE_INVERTED_INDEX",  # ç¨€ç–å‘é‡ä¸“ç”¨çš„å€’æ’ç´¢å¼•
                metric_type="IP",
            )

        # åˆ›å»ºé›†åˆ
        self.client.create_collection(
            collection_name=self.collection_name,
            schema=schema,
            index_params=index_params,
            enable_dynamic_field=True,
        )

    def insert_data(self, chunk, metadata):
        """
        æ’å…¥æ ‡å‡†æ•°æ®åˆ°Milvus
        
        ğŸ“¥ æ ‡å‡†æ•°æ®æ’å…¥æµç¨‹:
        è¿™æ˜¯åŸºç¡€çš„æ•°æ®æ’å…¥æ–¹æ³•ï¼Œå®ç°æœ€ç®€å•çš„æ–‡æœ¬å—å­˜å‚¨ç­–ç•¥
        
        ğŸ”„ å¤„ç†æµç¨‹:
        1. æ–‡æœ¬è¾“å…¥éªŒè¯
        2. å¯†é›†å‘é‡ç”Ÿæˆï¼ˆè¯­ä¹‰è¡¨ç¤ºï¼‰
        3. ç¨€ç–å‘é‡ç”Ÿæˆï¼ˆå¯é€‰ï¼Œå…³é”®è¯è¡¨ç¤ºï¼‰
        4. æ•°æ®ç»“æ„ç»„è£…
        5. Milvusæ‰¹é‡æ’å…¥
        
        ğŸ“Š æ•°æ®æµå‘:
        åŸå§‹æ–‡æœ¬å— â†’ Embeddingæ¨¡å‹ â†’ å‘é‡è¡¨ç¤º â†’ Milvuså­˜å‚¨
                                   â†“
        å…ƒæ•°æ®ä¿¡æ¯ â†’ å­—æ®µæ˜ å°„ â†’ åŠ¨æ€å­—æ®µå­˜å‚¨
        
        å‚æ•°:
            chunk: æ–‡æœ¬å—å†…å®¹ï¼ˆåŸå§‹æ–‡æœ¬ï¼Œæœªç»è¿‡ä¸Šä¸‹æ–‡å¢å¼ºï¼‰
            metadata: å…ƒæ•°æ®ï¼ˆåŒ…å«æ–‡æ¡£IDã€å—IDã€åŸå§‹å†…å®¹ç­‰ä¿¡æ¯ï¼‰
        
        å­˜å‚¨å†…å®¹ï¼š
        1. å¯†é›†å‘é‡ï¼šæ–‡æœ¬å—çš„è¯­ä¹‰å‘é‡è¡¨ç¤º
        2. ç¨€ç–å‘é‡ï¼ˆå¯é€‰ï¼‰ï¼šæ–‡æœ¬å—çš„å…³é”®è¯å‘é‡è¡¨ç¤º
        3. å…ƒæ•°æ®ï¼šæ–‡æ¡£å’Œå—çš„æ ‡è¯†ä¿¡æ¯
        """
        # ç”Ÿæˆæ–‡æœ¬å—çš„å¯†é›†å‘é‡è¡¨ç¤º
        dense_vec = self.embedding_function([chunk])[0]
        
        # æ„å»ºè¦æ’å…¥çš„æ•°æ®
        data = {
            "dense_vector": dense_vec,
            **metadata  # å±•å¼€å…ƒæ•°æ®å­—æ®µ
        }
        
        # å¦‚æœå¯ç”¨ç¨€ç–å‘é‡ï¼Œç”Ÿæˆå¹¶æ·»åŠ ç¨€ç–å‘é‡
        if self.use_sparse is True:
            sparse_vec = self.sparse_embedding_function([chunk])[0]
            data["sparse_vector"] = sparse_vec
            
        # æ’å…¥æ•°æ®åˆ°Milvusé›†åˆ
        self.client.insert(
            collection_name=self.collection_name,
            data=[data]
        )

    def insert_contextualized_data(self, doc_content, chunk_content, metadata):
        """
        æ’å…¥ä¸Šä¸‹æ–‡åŒ–çš„æ•°æ®
        
        ğŸ§  ä¸Šä¸‹æ–‡å¢å¼ºæ ¸å¿ƒæµç¨‹:
        è¿™æ˜¯å®ç°ä¸Šä¸‹æ–‡æ£€ç´¢çš„å…³é”®æ–¹æ³•ï¼Œè§£å†³ä¼ ç»ŸRAGçš„è¯­ä¹‰éš”ç¦»é—®é¢˜
        
                 ğŸ”„ ä¸Šä¸‹æ–‡åŒ–å¤„ç†æµç¨‹:
         1. æ–‡æ¡£ä¸Šä¸‹æ–‡å‡†å¤‡
         2. LLMæç¤ºè¯æ„å»º
         3. OpenAI GPT APIè°ƒç”¨ï¼ˆä¸Šä¸‹æ–‡å¢å¼ºï¼‰
         4. å¢å¼ºæ–‡æœ¬å‘é‡åŒ–
         5. å‘é‡æ•°æ®å­˜å‚¨
        
                 ğŸ“Š æ•°æ®å¢å¼ºæµç¨‹:
         åŸå§‹æ–‡æ¡£ â”€â”€â”
                   â”œâ”€â†’ LLMæç¤ºè¯ â†’ OpenAI GPT API â†’ ä¸Šä¸‹æ–‡å¢å¼ºæ–‡æœ¬
         æ–‡æœ¬å— â”€â”€â”€â”˜                                  â†“
                                             Embedding â†’ å¢å¼ºå‘é‡ â†’ Milvus
        
        ğŸ¯ æ ¸å¿ƒåˆ›æ–°ç‚¹:
        è¿™æ˜¯ä¸Šä¸‹æ–‡æ£€ç´¢çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
        1. ä½¿ç”¨æ•´ä¸ªæ–‡æ¡£å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        2. é€šè¿‡LLMï¼ˆOpenAI GPTï¼‰ä¸°å¯Œå•ä¸ªæ–‡æœ¬å—çš„è¯­ä¹‰ä¿¡æ¯
        3. ä½¿ç”¨å¢å¼ºåçš„æ–‡æœ¬ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
        
        âœ¨ ä¸Šä¸‹æ–‡åŒ–çš„ä¼˜åŠ¿ï¼š
        - è§£å†³æ–‡æœ¬å—å­¤ç«‹çš„é—®é¢˜
        - ä¿ç•™è·¨å—çš„è¯­ä¹‰è¿è´¯æ€§
        - æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦ä¸Šä¸‹æ–‡ç†è§£çš„æŸ¥è¯¢
        - å‡å°‘è¯­ä¹‰æ­§ä¹‰å’Œè¯¯è§£
        
        å‚æ•°:
            doc_content: æ•´ä¸ªæ–‡æ¡£å†…å®¹ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡èƒŒæ™¯ï¼‰
            chunk_content: å½“å‰æ–‡æœ¬å—å†…å®¹ï¼ˆéœ€è¦è¢«å¢å¼ºçš„éƒ¨åˆ†ï¼‰
            metadata: å…ƒæ•°æ®
        """
        # æ„å»ºLLMæç¤ºè¯ï¼Œè¦æ±‚ä¸ºæ–‡æœ¬å—æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡
        prompt = f"""
        <æ–‡æ¡£>
        {doc_content}
        </æ–‡æ¡£>
        <å—>
        {chunk_content}
        </å—>
        
        æˆ‘éœ€è¦ä½ å¯¹ä¸Šè¿°<å—>è¿›è¡Œä¸°å¯Œï¼Œä½¿ç”¨<æ–‡æ¡£>ä¸­çš„å†…å®¹æä¾›èƒŒæ™¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
        ä½ çš„å›ç­”åº”è¯¥åŒ…å«<å—>çš„å®Œæ•´å†…å®¹ï¼Œå¹¶ç¡®ä¿è¯­ä¹‰è¿è´¯ã€‚åªè¿”å›ä¸°å¯Œåçš„æ–‡æœ¬å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æˆ–è§£é‡Šã€‚
        
        ç›®æ ‡ï¼š
        1. ä¿æŒåŸå§‹å—çš„æ ¸å¿ƒä¿¡æ¯ä¸å˜
        2. æ·»åŠ å¿…è¦çš„èƒŒæ™¯ä¸Šä¸‹æ–‡ï¼Œä½¿å—çš„å«ä¹‰æ›´åŠ æ¸…æ™°
        3. ç¡®ä¿å¢å¼ºåçš„æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šæ˜¯è¿è´¯å’Œå®Œæ•´çš„
        """
        
        # === OpenAI GPT APIè°ƒç”¨ï¼ˆæ–°ç‰ˆæœ¬ï¼‰ ===
        # è°ƒç”¨OpenAI GPT APIç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„æ–‡æœ¬å—
        response = self.llm_client.chat.completions.create(
            model="gpt-3.5-turbo",  # ä½¿ç”¨GPT-3.5-turboï¼Œç»æµé«˜æ•ˆ
            # model="gpt-4",        # å¯é€‰æ‹©GPT-4è·å¾—æ›´å¥½æ•ˆæœ
            max_tokens=1000,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            temperature=0,          # ç¡®ä¿è¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é‡å¤æ€§
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # æå–OpenAIç”Ÿæˆçš„ä¸Šä¸‹æ–‡åŒ–æ–‡æœ¬
        contextualized_chunk = response.choices[0].message.content.strip()
        
        # === Claude APIè°ƒç”¨ï¼ˆåŸç‰ˆæœ¬ï¼Œå·²æ³¨é‡Šï¼‰ ===
        # # è°ƒç”¨Claude APIç”Ÿæˆä¸Šä¸‹æ–‡åŒ–çš„æ–‡æœ¬å—
        # message = self.anthropic_client.messages.create( 
        #     model="claude-3-haiku-20240307",  # ä½¿ç”¨å¿«é€Ÿã€ç»æµçš„Haikuæ¨¡å‹
        #     max_tokens=1000,  # é™åˆ¶è¾“å‡ºé•¿åº¦
        #     temperature=0,    # ç¡®ä¿è¾“å‡ºçš„ä¸€è‡´æ€§å’Œå¯é‡å¤æ€§
        #     messages=[
        #         {"role": "user", "content": prompt}
        #     ]
        # )
        # 
        # # æå–LLMç”Ÿæˆçš„ä¸Šä¸‹æ–‡åŒ–æ–‡æœ¬
        # contextualized_chunk = message.content[0].text.strip()
        
        # ä½¿ç”¨ä¸Šä¸‹æ–‡åŒ–åçš„å†…å®¹ç”ŸæˆåµŒå…¥å‘é‡å¹¶æ’å…¥
        dense_vec = self.embedding_function([contextualized_chunk])[0]
        data = {
            "dense_vector": dense_vec,
            "contextualized_content": contextualized_chunk,  # ä¿å­˜å¢å¼ºåçš„å†…å®¹
            **metadata
        }
        
        # å¦‚æœå¯ç”¨ç¨€ç–å‘é‡ï¼Œä¹Ÿä½¿ç”¨ä¸Šä¸‹æ–‡åŒ–å†…å®¹ç”Ÿæˆç¨€ç–å‘é‡
        if self.use_sparse is True:
            sparse_vec = self.sparse_embedding_function([contextualized_chunk])[0]
            data["sparse_vector"] = sparse_vec
            
        # æ’å…¥ä¸Šä¸‹æ–‡åŒ–æ•°æ®åˆ°Milvus
        self.client.insert(
            collection_name=self.collection_name,
            data=[data]
        )

    def search(self, query, k=5):
        """
        æœç´¢ç›¸å…³å†…å®¹
        
        ğŸ” æ™ºèƒ½æ£€ç´¢æ‰§è¡Œæµç¨‹:
        è¿™æ˜¯æ£€ç´¢ç³»ç»Ÿçš„æ ¸å¿ƒå…¥å£ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥çš„ç»Ÿä¸€è°ƒç”¨
        
        ğŸ¯ æ£€ç´¢ç­–ç•¥é€‰æ‹©:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    æ£€ç´¢é˜¶æ®µ     â”‚      å¤„ç†æ–¹å¼    â”‚      è¾“å‡ºç»“æœ    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 1. æŸ¥è¯¢å‘é‡åŒ–   â”‚ Embeddingæ¨¡å‹    â”‚ æŸ¥è¯¢å‘é‡è¡¨ç¤º     â”‚
        â”‚ 2. ç›¸ä¼¼åº¦æœç´¢   â”‚ Milvuså‘é‡æœç´¢   â”‚ Top-Kå€™é€‰ç»“æœ    â”‚
        â”‚ 3. ç»“æœé‡æ’åº   â”‚ Cohere Rerank    â”‚ ä¼˜åŒ–æ’åºç»“æœ     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        ğŸ”„ è¯¦ç»†æœç´¢æµç¨‹ï¼š
        1. æŸ¥è¯¢é¢„å¤„ç†ä¸å‘é‡åŒ–
        2. Milvuså‘é‡ç›¸ä¼¼åº¦æœç´¢
        3. åˆæ­¥ç»“æœè·å–ä¸è¿‡æ»¤
        4. å¯é€‰é‡æ’åºä¼˜åŒ–
        5. ç»“æœåå¤„ç†ä¸è¿”å›
        
        ğŸ“Š æœç´¢ä¼˜åŒ–ç­–ç•¥:
        æŸ¥è¯¢æ–‡æœ¬ â†’ å‘é‡åŒ– â†’ ç›¸ä¼¼åº¦æœç´¢ â†’ å€™é€‰ç»“æœ
                                            â†“
        æœ€ç»ˆç»“æœ â† é‡æ’åºä¼˜åŒ– â† è¯­ä¹‰åŒ¹é… â† ç»“æœé›†
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
        
        è¿”å›:
            æœç´¢ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
        """
        # è®¾ç½®æœç´¢å‚æ•°
        search_params = {"metric_type": "IP", "params": {"nprobe": 10}}
        
        # ç”ŸæˆæŸ¥è¯¢çš„åµŒå…¥å‘é‡
        dense_vec = self.embedding_function([query])[0]
        
        # æ‰§è¡Œæ ‡å‡†å¯†é›†å‘é‡æœç´¢
        # è¿™é‡Œä½¿ç”¨å†…ç§¯ï¼ˆIPï¼‰ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡
        res = self.client.search(
            collection_name=self.collection_name,
            data=[dense_vec],
            limit=k,
            output_fields=["content", "contextualized_content"],  # è¿”å›åŸå§‹å†…å®¹å’Œä¸Šä¸‹æ–‡åŒ–å†…å®¹
            search_params=search_params,
        )
        
        # ä½¿ç”¨é‡æ’åºå™¨è¿›ä¸€æ­¥ä¼˜åŒ–ç»“æœ
        # é‡æ’åºçš„ä½œç”¨ï¼šåŸºäºæŸ¥è¯¢å’Œæ–‡æ¡£çš„æ·±å±‚è¯­ä¹‰å…³ç³»é‡æ–°æ’åºç»“æœ
        if self.use_reranker:
            # æå–æ–‡æ¡£å†…å®¹ç”¨äºé‡æ’åº
            docs = []
            for hit in res[0]:
                # ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡åŒ–å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
                content = hit["entity"].get("contextualized_content", hit["entity"].get("content", ""))
                docs.append(content)
            
            # åº”ç”¨é‡æ’åºï¼šè®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªæ–‡æ¡£çš„æ·±å±‚ç›¸å…³æ€§åˆ†æ•°
            rerank_results = self.rerank_function(query, docs)
            
            # æ ¹æ®é‡æ’åºç»“æœé‡æ–°æ’åºåŸå§‹ç»“æœ
            reranked_results = []
            for result in rerank_results:
                idx = result.index  # ä½¿ç”¨ .index å±æ€§è·å–åŸå§‹ç´¢å¼•
                reranked_results.append(res[0][idx])
            
            res = [reranked_results]
        
        return res


def evaluate_retrieval(eval_data, retrieval_function, db, k=5):
    """
    è¯„ä¼°æ£€ç´¢æ€§èƒ½ - æ·±åº¦è¯„ä¼°ç³»ç»Ÿçš„æ ¸å¿ƒå‡½æ•°
    
    ğŸ“Š æ·±åº¦è¯„ä¼°ç³»ç»Ÿè®¾è®¡:
    è¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ£€ç´¢æ€§èƒ½è¯„ä¼°æ¡†æ¶ï¼Œé‡‡ç”¨å¤šç»´åº¦è¯„ä¼°ç­–ç•¥
    
    ğŸ”¬ æ·±åº¦è¯„ä¼°ï¼ˆDeep Evaluationï¼‰çš„æ¦‚å¿µï¼š
    1. ä¸ä»…ä»…è¯„ä¼°æ£€ç´¢çš„æ•°é‡ï¼Œæ›´å…³æ³¨æ£€ç´¢çš„è´¨é‡
    2. ä½¿ç”¨å¤šç»´åº¦æŒ‡æ ‡è¯„ä¼°æ£€ç´¢ç³»ç»Ÿæ€§èƒ½
    3. é€šè¿‡é»„é‡‘æ ‡å‡†æ•°æ®é›†è¿›è¡Œå®¢è§‚è¯„ä¼°
    4. æ”¯æŒä¸åŒæ£€ç´¢ç­–ç•¥çš„å…¬å¹³å¯¹æ¯”
    
    ğŸ¯ è¯„ä¼°æµç¨‹è®¾è®¡:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   è¯„ä¼°é˜¶æ®µ      â”‚    è¾“å…¥æ•°æ®     â”‚    å¤„ç†æ–¹å¼     â”‚    è¾“å‡ºç»“æœ     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. æ•°æ®å‡†å¤‡     â”‚ è¯„ä¼°æŸ¥è¯¢é›†      â”‚ æ ¼å¼éªŒè¯       â”‚ æ ‡å‡†åŒ–æ•°æ®     â”‚
    â”‚ 2. æ£€ç´¢æ‰§è¡Œ     â”‚ å•ä¸ªæŸ¥è¯¢       â”‚ æ£€ç´¢å‡½æ•°è°ƒç”¨    â”‚ å€™é€‰ç»“æœåˆ—è¡¨   â”‚
    â”‚ 3. ç»“æœåŒ¹é…     â”‚ æ£€ç´¢ç»“æœ       â”‚ ç²¾ç¡®æ–‡æœ¬åŒ¹é…    â”‚ åŒ¹é…æˆåŠŸè®¡æ•°   â”‚
    â”‚ 4. æ€§èƒ½è®¡ç®—     â”‚ åŒ¹é…ç»Ÿè®¡       â”‚ æŒ‡æ ‡è®¡ç®—       â”‚ è¯„ä¼°åˆ†æ•°      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯´æ˜ï¼š
    - Pass@K: åœ¨å‰Kä¸ªæ£€ç´¢ç»“æœä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æŸ¥è¯¢æ¯”ä¾‹
      * è®¡ç®—å…¬å¼: (åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æŸ¥è¯¢æ•° / æ€»æŸ¥è¯¢æ•°) Ã— 100%
      * åæ˜ ç³»ç»Ÿçš„æ•´ä½“æ£€ç´¢æˆåŠŸç‡
    - å¹³å‡åˆ†æ•°: æ¯ä¸ªæŸ¥è¯¢æ£€ç´¢åˆ°çš„æ­£ç¡®æ–‡æ¡£å—å æ€»æ­£ç¡®å—çš„æ¯”ä¾‹
      * è®¡ç®—å…¬å¼: Î£(æŸ¥è¯¢æ£€ç´¢åˆ°çš„æ­£ç¡®å—æ•° / æŸ¥è¯¢çš„æ€»æ­£ç¡®å—æ•°) / æ€»æŸ¥è¯¢æ•°
      * åæ˜ ç³»ç»Ÿçš„ç»†ç²’åº¦æ£€ç´¢ç²¾åº¦
    - å¬å›ç‡: è¯„ä¼°ç³»ç»Ÿæ‰¾åˆ°ç›¸å…³æ–‡æ¡£çš„èƒ½åŠ›
      * è¡¡é‡ç³»ç»Ÿåœ¨ä¸é—æ¼é‡è¦ä¿¡æ¯æ–¹é¢çš„è¡¨ç°
    
    ğŸ”„ è¯„ä¼°æ‰§è¡Œæµç¨‹:
    è¯„ä¼°æ•°æ® â†’ æŸ¥è¯¢è§£æ â†’ é»„é‡‘æ ‡å‡†æå– â†’ æ£€ç´¢æ‰§è¡Œ â†’ ç»“æœå¯¹æ¯” â†’ æŒ‡æ ‡è®¡ç®—
        â†“
    æ€§èƒ½æŠ¥å‘Š â† ç»Ÿè®¡åˆ†æ â† åˆ†æ•°æ±‡æ€» â† åŒ¹é…éªŒè¯ â† ç²¾ç¡®åŒ¹é…
    
    å‚æ•°:
        eval_data: è¯„ä¼°æ•°æ®é›†
            - åŒ…å«æŸ¥è¯¢å’Œå¯¹åº”çš„é»„é‡‘æ ‡å‡†ç­”æ¡ˆ
            - æ¯ä¸ªæŸ¥è¯¢éƒ½æœ‰æ˜ç¡®çš„æ­£ç¡®æ–‡æ¡£å—æ ‡è¯†
        retrieval_function: æ£€ç´¢å‡½æ•°
            - æ¥å—æŸ¥è¯¢å’Œæ•°æ®åº“ï¼Œè¿”å›æ£€ç´¢ç»“æœ
        db: æ•°æ®åº“å®ä¾‹
        k: è¯„ä¼°çš„top-kç»“æœæ•°
        
    è¿”å›:
        è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«Pass@Kåˆ†æ•°ã€å¹³å‡åˆ†æ•°ç­‰æŒ‡æ ‡
    """
    total_score = 0      # ç´¯è®¡åˆ†æ•°
    total_queries = 0    # æ€»æŸ¥è¯¢æ•°
    
    # éå†æ¯ä¸ªè¯„ä¼°æŸ¥è¯¢
    for item in tqdm(eval_data, desc="Evaluating retrieval"):
        total_queries += 1
        query = item["query"]
        
        # è·å–å½“å‰æŸ¥è¯¢çš„é»„é‡‘æ ‡å‡†å†…å®¹ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰
        golden_contents = []
        for ref in item["references"]:
            doc_uuid = ref["doc_uuid"]      # æ–‡æ¡£çš„å”¯ä¸€æ ‡è¯†
            chunk_index = ref["chunk_index"] # æ–‡æ¡£å—çš„ç´¢å¼•
            
            # åœ¨æ•°æ®é›†ä¸­æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ–‡æ¡£
            golden_doc = next(
                (
                    doc
                    for doc in dataset
                    if doc.get("original_uuid") == doc_uuid
                ),
                None,
            )
            if not golden_doc:
                print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°UUIDä¸º{doc_uuid}çš„é»„é‡‘æ–‡æ¡£")
                continue
                
            # åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£å—
            golden_chunk = next(
                (
                    chunk
                    for chunk in golden_doc["chunks"]
                    if chunk["original_index"] == chunk_index
                ),
                None,
            )
            if not golden_chunk:
                print(f"è­¦å‘Šï¼šåœ¨æ–‡æ¡£{doc_uuid}ä¸­æœªæ‰¾åˆ°ç´¢å¼•ä¸º{chunk_index}çš„é»„é‡‘å—")
                continue
                
            golden_contents.append(golden_chunk["content"].strip())
            
        if not golden_contents:
            print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°æŸ¥è¯¢çš„é»„é‡‘å†…å®¹ï¼š{query}")
            continue
            
        # ä½¿ç”¨æ£€ç´¢å‡½æ•°è·å–æ£€ç´¢ç»“æœ
        retrieved_docs = retrieval_function(query, db, k=k)
        
        # è®¡ç®—æœ‰å¤šå°‘é»„é‡‘å—åœ¨top-kæ£€ç´¢æ–‡æ¡£ä¸­è¢«æ‰¾åˆ°
        # è¿™æ˜¯è¯„ä¼°æ£€ç´¢å‡†ç¡®æ€§çš„æ ¸å¿ƒé€»è¾‘
        chunks_found = 0
        for golden_content in golden_contents:
            for doc in retrieved_docs[0][:k]:  # åªæ£€æŸ¥å‰kä¸ªç»“æœ
                content_field = "content"
                if "contextualized_content" in doc["entity"]:
                    # è¯„ä¼°æ—¶ä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿å…¬å¹³æ€§
                    content_field = "content"
                retrieved_content = doc["entity"][content_field].strip()
                
                # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
                if retrieved_content == golden_content:
                    chunks_found += 1
                    break  # æ‰¾åˆ°åŒ¹é…å°±è·³å‡ºå†…å±‚å¾ªç¯
                    
        # è®¡ç®—å½“å‰æŸ¥è¯¢çš„åˆ†æ•°ï¼šæ‰¾åˆ°çš„æ­£ç¡®å—æ•° / æ€»çš„æ­£ç¡®å—æ•°
        query_score = chunks_found / len(golden_contents)
        total_score += query_score
        
    # è®¡ç®—æ•´ä½“è¯„ä¼°æŒ‡æ ‡
    average_score = total_score / total_queries
    pass_at_n = average_score * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    return {
        "pass_at_n": pass_at_n,           # Pass@Kåˆ†æ•°ï¼ˆç™¾åˆ†æ¯”ï¼‰
        "average_score": average_score,    # å¹³å‡åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        "total_queries": total_queries,    # æ€»æŸ¥è¯¢æ•°
    }


def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:
    """
    åŸºç¡€æ£€ç´¢å‡½æ•°
    
    è¿™æ˜¯ä¸€ä¸ªç®€å•çš„åŒ…è£…å‡½æ•°ï¼Œè°ƒç”¨æ•°æ®åº“çš„æœç´¢æ–¹æ³•
    ç”¨äºè¯„ä¼°ç³»ç»Ÿä¸­çš„ç»Ÿä¸€æ¥å£
    """
    return db.search(query, k=k)


def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½JSONLæ–‡ä»¶å¹¶è¿”å›å­—å…¸åˆ—è¡¨
    
    JSONLæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼Œé€‚åˆå­˜å‚¨ç»“æ„åŒ–çš„è¯„ä¼°æ•°æ®
    """
    with open(file_path, "r") as file:
        return [json.loads(line) for line in file]


def evaluate_db(db, original_jsonl_path: str, k):
    """
    è¯„ä¼°æ•°æ®åº“çš„æ£€ç´¢æ€§èƒ½
    
    è¿™æ˜¯è¯„ä¼°æµç¨‹çš„ä¸»å…¥å£å‡½æ•°ï¼š
    1. åŠ è½½è¯„ä¼°æ•°æ®é›†
    2. è¿è¡Œæ£€ç´¢è¯„ä¼°
    3. è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
    
    å‚æ•°:
        db: è¦è¯„ä¼°çš„æ•°æ®åº“å®ä¾‹
        original_jsonl_path: è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        k: è¯„ä¼°çš„top-kå‚æ•°
    
    è¿”å›:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    # åŠ è½½åŸå§‹JSONLæ•°æ®ä½œä¸ºæŸ¥è¯¢å’ŒçœŸå®æ ‡ç­¾
    original_data = load_jsonl(original_jsonl_path)
    
    # è¯„ä¼°æ£€ç´¢æ€§èƒ½
    results = evaluate_retrieval(original_data, retrieve_base, db, k)
    
    # è¾“å‡ºè¯„ä¼°ç»“æœ
    print(f"Pass@{k}: {results['pass_at_n']:.2f}%")
    print(f"æ€»åˆ†: {results['average_score']}")
    print(f"æ€»æŸ¥è¯¢æ•°: {results['total_queries']}")
    
    return results


def download_data():
    """
    ä¸‹è½½ç¤ºä¾‹æ•°æ®
    
    ä»Anthropicçš„GitHubä»“åº“ä¸‹è½½æ¼”ç¤ºæ•°æ®ï¼š
    1. codebase_chunks.json: ä»£ç åº“çš„æ–‡æ¡£å—æ•°æ®
    2. evaluation_set.jsonl: è¯„ä¼°æŸ¥è¯¢å’Œæ ‡å‡†ç­”æ¡ˆ
    """
    import urllib.request
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
    if not os.path.exists("codebase_chunks.json"):
        print("ä¸‹è½½codebase_chunks.json...")
        urllib.request.urlretrieve(
            "https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/codebase_chunks.json",
            "codebase_chunks.json"
        )
    
    if not os.path.exists("evaluation_set.jsonl"):
        print("ä¸‹è½½evaluation_set.jsonl...")
        urllib.request.urlretrieve(
            "https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/evaluation_set.jsonl",
            "evaluation_set.jsonl"
        )
    
    print("æ•°æ®ä¸‹è½½å®Œæˆï¼")


def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰å®éªŒ
    
    ğŸ§ª å®éªŒè®¾è®¡æ¡†æ¶:
    è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¯¹æ¯”å®éªŒç³»ç»Ÿï¼Œé‡‡ç”¨æ§åˆ¶å˜é‡æ³•éªŒè¯ä¸åŒæ£€ç´¢ç­–ç•¥çš„æ•ˆæœ
    
    ğŸ¯ å®éªŒç›®æ ‡:
    - éªŒè¯ä¸Šä¸‹æ–‡æ£€ç´¢ç›¸å¯¹äºæ ‡å‡†æ£€ç´¢çš„æ€§èƒ½æå‡
    - é‡åŒ–é‡æ’åºæŠ€æœ¯çš„é¢å¤–æ”¶ç›Š
    - å»ºç«‹æ£€ç´¢æŠ€æœ¯çš„æ€§èƒ½åŸºå‡†
    - ä¸ºå®é™…åº”ç”¨æä¾›æŠ€æœ¯é€‰æ‹©æŒ‡å¯¼
    
    ğŸ“Š å®éªŒè®¾è®¡çŸ©é˜µ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     å®éªŒç»„      â”‚ æ–‡æœ¬é¢„å¤„ç† â”‚ å‘é‡æ£€ç´¢   â”‚ ç»“æœé‡æ’åº â”‚  é¢„æœŸæ€§èƒ½    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ ‡å‡†æ£€ç´¢(åŸºçº¿)  â”‚ åŸå§‹æ–‡æœ¬å— â”‚ å¯†é›†å‘é‡   â”‚ æ—         â”‚ åŸºå‡†æ€§èƒ½     â”‚
    â”‚ ä¸Šä¸‹æ–‡æ£€ç´¢      â”‚ LLMå¢å¼ºå—  â”‚ å¯†é›†å‘é‡   â”‚ æ—         â”‚ ä¸­ç­‰æå‡     â”‚
    â”‚ é‡æ’åºæ£€ç´¢      â”‚ LLMå¢å¼ºå—  â”‚ å¯†é›†å‘é‡   â”‚ Cohere    â”‚ æœ€ä½³æ€§èƒ½     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ”¬ å®éªŒæ§åˆ¶å˜é‡:
    - ç›¸åŒçš„æ•°æ®é›†ï¼ˆcodebase_chunks.jsonï¼‰
    - ç›¸åŒçš„è¯„ä¼°æŸ¥è¯¢ï¼ˆevaluation_set.jsonlï¼‰
    - ç›¸åŒçš„åµŒå…¥æ¨¡å‹ï¼ˆBGE-large-zhï¼‰
    - ç›¸åŒçš„è¯„ä¼°æŒ‡æ ‡ï¼ˆPass@5ï¼‰
    - ç›¸åŒçš„å‘é‡æ•°æ®åº“é…ç½®
    
    ğŸš€ å®éªŒæ‰§è¡Œæµç¨‹:
    1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡é˜¶æ®µ:
       - APIå¯†é’¥é…ç½®éªŒè¯
       - æ¨¡å‹åˆå§‹åŒ–å’Œæµ‹è¯•
       - æ•°æ®ä¸‹è½½å’ŒéªŒè¯
    
    2ï¸âƒ£ æ•°æ®é¢„å¤„ç†é˜¶æ®µ:
       - æ–‡æ¡£æ•°æ®åŠ è½½å’Œè§£æ
       - è¯„ä¼°æŸ¥è¯¢é›†æ„å»º
       - æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
    
    3ï¸âƒ£ å®éªŒæ‰§è¡Œé˜¶æ®µ:
       - æ ‡å‡†æ£€ç´¢å®éªŒï¼ˆåŸºçº¿ï¼‰
       - ä¸Šä¸‹æ–‡æ£€ç´¢å®éªŒï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
       - é‡æ’åºæ£€ç´¢å®éªŒï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
    
    4ï¸âƒ£ ç»“æœåˆ†æé˜¶æ®µ:
       - æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
       - æ”¹è¿›å¹…åº¦è®¡ç®—
       - å®éªŒç»“è®ºæ€»ç»“
    
    ğŸ¯ æ ¸å¿ƒå®éªŒå‡è®¾:
    è¿™ä¸ªå‡½æ•°è¿è¡Œä¸‰ä¸ªå¯¹æ¯”å®éªŒï¼Œå±•ç¤ºä¸åŒæ£€ç´¢ç­–ç•¥çš„æ€§èƒ½å·®å¼‚ï¼š
    
    1. æ ‡å‡†æ£€ç´¢ï¼šåŸºçº¿æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬å—
    2. ä¸Šä¸‹æ–‡æ£€ç´¢ï¼šä½¿ç”¨LLMå¢å¼ºæ–‡æœ¬å—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    3. å¸¦é‡æ’åºçš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼šåœ¨ä¸Šä¸‹æ–‡æ£€ç´¢åŸºç¡€ä¸ŠåŠ å…¥é‡æ’åºä¼˜åŒ–
    
    âš–ï¸ å®éªŒå…¬å¹³æ€§ä¿è¯:
    æ¯ä¸ªå®éªŒéƒ½ä½¿ç”¨ç›¸åŒçš„è¯„ä¼°æ•°æ®é›†ï¼Œç¡®ä¿æ¯”è¾ƒçš„å…¬å¹³æ€§
    """
    # æ›¿æ¢è¿™äº›ä¸ºä½ çš„å®é™…APIå¯†é’¥
    cohere_api_key = os.getenv("COHERE_API_KEY")      # Cohereé‡æ’åºAPIå¯†é’¥
    # anthropic_api_key = os.getenv("CLAUDE_API_KEY")   # Claude APIå¯†é’¥ï¼ˆå·²æ³¨é‡Šï¼‰
    openai_api_key = os.getenv("OPENAI_API_KEY")      # OpenAI APIå¯†é’¥ï¼ˆæ–°å¢ï¼‰
    
    # ä¸‹è½½ç¤ºä¾‹æ•°æ®
    download_data()
    
    # åŠ è½½æ•°æ®é›†
    global dataset
    with open("codebase_chunks.json", "r") as f:
        dataset = json.load(f)
    
    # åªä½¿ç”¨å‰5ä¸ªæ–‡æ¡£è¿›è¡Œæµ‹è¯•ï¼ˆå‡å°‘APIè°ƒç”¨æˆæœ¬å’Œè¿è¡Œæ—¶é—´ï¼‰
    dataset = dataset[:5]
    
    # åˆå§‹åŒ–å„ç§æ¨¡å‹å’Œå‡½æ•°
    dense_ef = SentenceTransformerEmbeddingFunction(model_name='BAAI/bge-large-zh')  # ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„BGEæ¨¡å‹
    cohere_rf = CohereRerankFunction(api_key=cohere_api_key)  # Cohereé‡æ’åºå‡½æ•°
    
    # === OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆæ–°ç‰ˆæœ¬ï¼‰ ===
    openai_client = openai.OpenAI(api_key=openai_api_key)  # OpenAIå®¢æˆ·ç«¯
    
    # === Claudeå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼ˆåŸç‰ˆæœ¬ï¼Œå·²æ³¨é‡Šï¼‰ ===
    # anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)  # Claudeå®¢æˆ·ç«¯
    
    # ===============================
    # å®éªŒä¸€ï¼šæ ‡å‡†æ£€ç´¢ï¼ˆåŸºçº¿æ–¹æ³•ï¼‰
    # ===============================
    print("\n===== å®éªŒä¸€ï¼šæ ‡å‡†æ£€ç´¢ =====")
    print("è¿™æ˜¯åŸºçº¿å®éªŒï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬å—è¿›è¡Œæ£€ç´¢ï¼Œæ²¡æœ‰ä»»ä½•å¢å¼º")
    
    standard_retriever = MilvusContextualRetriever(
        uri="standard.db", 
        collection_name="standard", 
        dense_embedding_function=dense_ef
    )
    
    # æ„å»ºé›†åˆå¹¶æ’å…¥æ ‡å‡†æ•°æ®
    standard_retriever.build_collection()
    for doc in tqdm(dataset, desc="æ’å…¥æ ‡å‡†æ£€ç´¢æ•°æ®"):
        doc_content = doc["content"]
        for chunk in doc["chunks"]:
            metadata = {
                "doc_id": doc["doc_id"],
                "original_uuid": doc["original_uuid"],
                "chunk_id": chunk["chunk_id"],
                "original_index": chunk["original_index"],
                "content": chunk["content"],
            }
            chunk_content = chunk["content"]
            standard_retriever.insert_data(chunk_content, metadata)
    
    # åˆ›å»ºç®€åŒ–çš„è¯„ä¼°æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„è¯„ä¼°æ•°æ®é›†
    eval_data = []
    for doc in dataset[:2]:  # åªä½¿ç”¨å‰2ä¸ªæ–‡æ¡£è¿›è¡Œè¯„ä¼°
        for chunk in doc["chunks"][:2]:  # æ¯ä¸ªæ–‡æ¡£åªå–å‰2ä¸ªå—
            eval_data.append({
                "query": chunk["content"][:50],  # ä½¿ç”¨å—å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæŸ¥è¯¢
                "references": [{
                    "doc_uuid": doc["original_uuid"],
                    "chunk_index": chunk["original_index"]
                }]
            })
    
    # ä¿å­˜è¯„ä¼°æ•°æ®
    with open("evaluation_set.jsonl", "w") as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # è¯„ä¼°æ ‡å‡†æ£€ç´¢æ€§èƒ½
    standard_results = evaluate_db(standard_retriever, "evaluation_set.jsonl", 5)
    
    # ===============================
    # å®éªŒäºŒï¼šä¸Šä¸‹æ–‡æ£€ç´¢
    # ===============================
    print("\n===== å®éªŒäºŒï¼šä¸Šä¸‹æ–‡æ£€ç´¢ =====")
    print("ä½¿ç”¨OpenAI GPTä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œè§£å†³è¯­ä¹‰éš”ç¦»é—®é¢˜")
    
    contextual_retriever = MilvusContextualRetriever(
        uri="contextual.db",
        collection_name="contextual",
        dense_embedding_function=dense_ef,
        use_contextualize_embedding=True,  # å¯ç”¨ä¸Šä¸‹æ–‡åŒ–
        llm_client=openai_client,  # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
        # anthropic_client=anthropic_client,  # åŸClaudeå®¢æˆ·ç«¯ï¼ˆå·²æ³¨é‡Šï¼‰
    )
    
    # æ„å»ºé›†åˆå¹¶æ’å…¥ä¸Šä¸‹æ–‡åŒ–æ•°æ®
    contextual_retriever.build_collection()
    for doc in tqdm(dataset, desc="æ’å…¥ä¸Šä¸‹æ–‡æ£€ç´¢æ•°æ®"):
        doc_content = doc["content"]
        for chunk in doc["chunks"]:
            metadata = {
                "doc_id": doc["doc_id"],
                "original_uuid": doc["original_uuid"],
                "chunk_id": chunk["chunk_id"],
                "original_index": chunk["original_index"],
                "content": chunk["content"],
            }
            chunk_content = chunk["content"]
            # ä½¿ç”¨ä¸Šä¸‹æ–‡åŒ–æ’å…¥æ–¹æ³•
            contextual_retriever.insert_contextualized_data(
                doc_content, chunk_content, metadata
            )
    
    # è¯„ä¼°ä¸Šä¸‹æ–‡æ£€ç´¢æ€§èƒ½
    contextual_results = evaluate_db(contextual_retriever, "evaluation_set.jsonl", 5)
    
    # ===============================
    # å®éªŒä¸‰ï¼šå¸¦é‡æ’åºçš„ä¸Šä¸‹æ–‡æ£€ç´¢
    # ===============================
    print("\n===== å®éªŒä¸‰ï¼šå¸¦é‡æ’åºçš„ä¸Šä¸‹æ–‡æ£€ç´¢ =====")
    print("åœ¨ä¸Šä¸‹æ–‡æ£€ç´¢åŸºç¡€ä¸Šï¼Œä½¿ç”¨Cohereé‡æ’åºæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–ç»“æœ")
    
    # å¯ç”¨é‡æ’åºåŠŸèƒ½
    contextual_retriever.use_reranker = True
    contextual_retriever.rerank_function = cohere_rf
    
    # è¯„ä¼°å¸¦é‡æ’åºçš„æ£€ç´¢æ€§èƒ½
    reranker_results = evaluate_db(contextual_retriever, "evaluation_set.jsonl", 5)
    
    # ===============================
    # ç»“æœå¯¹æ¯”åˆ†æ
    # ===============================
    print("\n===== æ‰€æœ‰å®éªŒç»“æœæ¯”è¾ƒ =====")
    print("æ€§èƒ½æå‡åˆ†æï¼š")
    print(f"æ ‡å‡†æ£€ç´¢ Pass@5: {standard_results['pass_at_n']:.2f}%")
    print(f"ä¸Šä¸‹æ–‡æ£€ç´¢ Pass@5: {contextual_results['pass_at_n']:.2f}%")
    print(f"å¸¦é‡æ’åºçš„ä¸Šä¸‹æ–‡æ£€ç´¢ Pass@5: {reranker_results['pass_at_n']:.2f}%")
    
    # è®¡ç®—æ”¹è¿›å¹…åº¦
    context_improvement = contextual_results['pass_at_n'] - standard_results['pass_at_n']
    rerank_improvement = reranker_results['pass_at_n'] - standard_results['pass_at_n']
    
    print(f"\næ€§èƒ½æ”¹è¿›åˆ†æï¼š")
    print(f"ä¸Šä¸‹æ–‡æ£€ç´¢ç›¸æ¯”æ ‡å‡†æ£€ç´¢æå‡: {context_improvement:.2f}ä¸ªç™¾åˆ†ç‚¹")
    print(f"é‡æ’åºè¿›ä¸€æ­¥æå‡: {rerank_improvement:.2f}ä¸ªç™¾åˆ†ç‚¹")
    print(f"æ€»ä½“æå‡: {rerank_improvement:.2f}ä¸ªç™¾åˆ†ç‚¹")


if __name__ == "__main__":
    main()
