# 基础知识

## DSS

决策支持系统（Decision Support System）

在体系结构化环境的核心，主要存在两种数据：原始数据和导出数据。下面是其区别

|                  原始数据/操作型数据                   |    导出数据/DSS型数据    |
| :----------------------------------------------------: | :----------------------: |
|                      **面向应用**                      |       **面向主题**       |
|                       **详细的**                       |   **概要的或精细化的**   |
|                 **在访问瞬间是准确的**                 | **代表过去的数据，快照** |
|                   **为日常工作服务**                   |     **为管理者服务**     |
|                         可更新                         |          不更新          |
|                        重复运行                        |        启发式运行        |
|                    处理需求预先可知                    |    处理需求事先不知道    |
|                   生命周期符合 SDLC                    |    完全不同的生命周期    |
|                      对性能要求高                      |      对性能要求宽松      |
|                    一次访问一个单元                    |     一次访问一个集合     |
|                      事务处理驱动                      |       分析处理驱动       |
| 就操作型数据库更新责任来说更新控制是一个主要关心的问题 |     无更新控制的问题     |
|                        高可用性                        |     宽松的可用性要求     |
|                        整体管理                        |        以子集管理        |
|                        非冗余性                        |       总是存在冗余       |
|                  静态结构；可变的内容                  |         结构灵活         |
|                    一次处理数据量小                    |     一次处理数据量大     |
|                      支持日常操作                      |       支持管理需求       |
|                        访问频繁                        |      访问很少或不多      |

下面是其他的差异

-  原始数据是维持企业日常运行所需的细节性数据；导出数据是要经过汇总或计算来满足公司管理者需要的数据。
-  原始数据可以更新；导出数据可以重新计算得出，但不能直接进行更新。
-  原始数据主要是当前值数据；导出数据通常为历史数据。
-  原始数据由以重复方式运行的过程操作；导出数据由启发式而非重复地运行的程序与过程操作。
-  操作型数据是原始的；DSS 数据是导出的。
-  原始数据支持日常工作；导出数据则支持管理工作。

**两者不能存在同一数据库中，甚至不能共存于同一个环境中**。

## ODS

[源自](https://sqlwithmanoj.com/2014/12/17/what-is-ods-operational-data-store-and-how-it-differs-from-data-warehouse-dw/)

> **Simple definition**
>
> An Operational Data Store (ODS) is a module in the Data Warehouse that contains the most latest snapshot of **Operational Data**. It is designed to contain atomic or low-level data with limited history for “Real Time” or “Near Real Time” (NRT) reporting on frequent basis.
>
> **Detailed definifion**
>
> - An ODS is basically a database that is used for being an interim area for a data warehouse (DW), it sits between the legacy systems environment and the DW.
>
> - It works with a Data Warehouse (DW) but unlike a DW, an ODS does not contain Static data. Instead, an ODS contains data which is dynamically and constantly updated through the various course of the Business Actions and Operations.
>
> - It is specifically designed so that it can Quickly perform simpler queries on smaller sets of data.
>
> - This is in contrast to the structure of DW wherein it needs to perform complex queries on large sets of data.
>
> - As the Data ages in ODS it passes out of the DW environment as it is.

简单翻译一下

**简单定义**

ODS 是数仓中一个模块（这里其实是有歧义的），它包含了操作性数据的最近快照。它被设计为包含原子性或低层级的数据，这些数据具有有限的历史记录，经常用于“实时”或“近实时”（NRT）上报数据。

**详细定义**

-   ODS 基本上是一个数据库，用作数据仓库（DW）的过渡区域，它位于旧系统环境和数仓之间。
 -   它与数仓一起使用，但是与数仓不同，ODS 不包含静态数据。 取而代之的是，ODS 包含通过业务活动和运营的各个过程动态且不断更新的数据。
 -   它经过专门设计，因此可以对较小的数据集快速执行更简单的查询。
-   这与数仓的结构形成对比，数仓的结构需要对大数据集执行复杂的查询。
-   随着数据在 ODS 中老化，它会照原样从数仓环境中传出。

**补充**

操作型数据存储（Operational Data Store，ODS）：ODS 存放了数据中心需要用到的业务明细数据，通常是一个可选项，用来在业务系统和数据仓库之间形成一个隔离层。ODS 中的明细数据将被进一步加工、集成和汇总，并加载到数据仓库或数据集市中。

![](D:\数仓\PNG\what-is-operational-data-stores.png)

### 从操作环境到 ODS 的四个时间

在操作环境到 ODS 的时间，一般有四个。

1. 实时——从操作环境到 ODS 的数据更新是同步进行的。
2. 准实时——从操作环境到 ODS 的数据更新有 2~3 小时间隔。
3. T+1——从操作环境到 ODS 的数据更新夜间完成的。
4. 随机——从数据仓库到这类 ODS 的更新是不预先规划的

## 一致性保障

1. 至多一次语义（At-most-once Semantics）：消息或事件对应用状态的影响最多只有一次。
2. 至少一次语义（At-least-once Semantics）：消息或事件对应用状态的影响最少一次。
3. 精确一次语义（Exactly-once Semantics，有些地方会简称 EOS）：消息或事件对应用状态的影响有且只有一次。

下面介绍对应的实时数仓内容的框架时会提到。

### Kafka 的 EOS 实现

简单来说，Kafka 的精准一次语义有两种机制实现：幂等性（Idempotence）和事务（Transaction）。

**幂等**

“幂等”这个词原是数学领域中的概念，指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。

设置幂等性 Producer，`props.put(“enable.idempotence”, ture)`，Kafka 自动帮你做消息的重复去重。底层具体的原理很简单，就是经典的用空间去换时间的优化思路，即在 Broker 端多保存一些字段。当 Producer 发送了具有相同字段值的消息后，Broker 能够自动知晓这些消息已经重复了，于是可以在后台默默地把它们“丢弃”掉。当然，实际的实现原理并没有这么简单，但你大致可以这么理解。

一个幂等性 Producer 能够保证某个主题的一个分区上不出现重复消息，它无法实现多个分区的幂等性。其次，它只能实现单会话上的幂等性，不能实现跨会话的幂等性。这里的会话，你可以理解为 Producer 进程的一次运行。当你重启了 Producer 进程之后，这种幂等性保证就丧失了。

**事务**

Kafka 实现了 RC（Read Committed 读已提交）级别，即保证多条消息原子性地写入到目标分区，同时也能保证  Consumer 只能看到事务成功提交的消息。

需要设置 `enable.idempotence = true`，还需要在 Producer 端显式地开启和关闭事务，并且还需要 Consumer 端设置 `isolation.level=read_committed`才能实现 Kafka 事务。

## 数据形式

数据根据形式，可以分为三种数据

1. 结构化数据——即**行数据**，存储在数据库里，可以用二维表结构来逻辑表达实现的数据。例如 MySQL、Oracle 中存储的数据。
2. 半结构化数据——半结构化数据是结构化数据的一种形式，它并不符合关系型数据库或其他数据表的形式关联起来的数据模型结构，但包含相关标记，用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的结构。例如 **XML、JSON、Excel、文本和日志**等。
3. 非结构化数据——非结构化数据是数据结构不规则或不完整，没有预定义的数据模型，不方便用数据库二维逻辑表来表现的数据。包括所有格式的**办公文档、文本、图片、各类报表、图像和音频/视频信息**等等。

## 批处理和流处理

批处理是一种非常古老的计算方式。早在可编程数字计算机诞生之前，打孔卡制表机（例如1890年美国人口普查中使用的霍尔里斯机）实现了半机械化的批处理形式，从大量输入中汇总计算。  Map-Reduce与1940年代和1950年代广泛用于商业数据处理的机电IBM卡片分类机器有着惊人的相似之处。正如我们所说，历史总是在不断重复自己<sup>《数据密集型应用系统设计》</sup>。

在 Flink 中，有界数据流（**Unbounded streams** ）处理就是批处理，无界数据流（**Bounded streams**）处理就是流处理。

下面更为直观的图。
![](https://static001.geekbang.org/resource/image/2f/b2/2f8e72ce532cf1d05306cb8b78510bb2.png)这是官网的图
![Bounded vs Unbounded](https://nightlies.apache.org/flink/flink-docs-release-1.13/fig/learn-flink/bounded-unbounded.png)

## DAG

所谓 DAG 也就是有向无环图（Directed Acyclic Graph ），就是说不同阶段的依赖关系是有向的，计算过程只能沿着依赖关系方向执行，被依赖的阶段执行完成之前，依赖的阶段不能开始执行，同时，这个依赖关系不能有环形依赖，否则就成为死循环了。

下图 Flink 的 Job Graph（作业图）例子。

![Parallel Job](https://nightlies.apache.org/flink/flink-docs-release-1.13/fig/learn-flink/parallel-job.png)

下图的**数据开发**模块就是 DAG 的一个例子。

![流计算开发套件.png](https://z3.ax1x.com/2021/11/15/I2IRoj.png)

这个在大数据中会经常提到，Hive 或者 Spark 的 SQL 转换成 MapReduce 任务会生成 DAG。如果深入了解 MySQL 这种关系型数据库，理解 Hive 之类的框架，其实不难。SQL 在语法分析、语法解析、语法优化、<权限检查>之后，会转成抽象语法树、语义检查、树转执行计划、并进行评估最优，再由执行引擎（如 InnoDB）执行计划，很多数据库都是这么设计的。如下图所示

![数仓相关内容-MySQL-Architecture.png](https://z3.ax1x.com/2021/11/15/I2o9OO.png)

**树就是一种特殊的图**。

## 事实表（Fact Table）、维表（Dimension Table）

### 维表

维度表包含通常是文本字段的描述性属性。 这些属性旨在服务于两个关键目的：查询约束和/或过滤，以及查询结果集标签。 

维度表是维度建模的灵魂，在维度表设计中碰到的问题（如维度变化、维度层次、维度一致性、维度整合和拆分等）直接关系到维度建模的好坏。

### 事实表

事实表通常有两种类型的列：包含事实的列和作为维度表外键的列。 事实表的主键通常是由其所有外键组成的复合键。 事实表包含数据仓库的内容并存储不同类型的度量，如加性、非加性和半加性度量。

事实表是维度建模的核心表和基本表。事实表存储了业务过程中的各种度量和事实，而这些度量和事实正式下游数据使用人员所要关心和分析的对象。

事实表主要有三种类型，分别是**事务事实表、快照事实表和累计快照事实表**。还有个特殊的事实表——**无事实的事实表**。

1. **事务事实表**

   事务事实表通常用于记录业务过程的事件，而且是原子粒度的事件。事务事实表中的数据在事务事件发生后产生，数据的粒度通常是每个事务一条记录。一旦事务被提交，事实表数据被插入，数据就不再进行更改。通过事务事实表存储单次业务事件/行为的细节，以及存储与事件相关的维度细节，用户即可单独或者聚合分析业务实践和行为。

   事务事实表的粒度确定是事务事实表设计过程中的关键步骤，一般都会包含可加的度量和事实。

2. **快照事实表**
   在实际的业务活动中，除了关心单次的业务实践和行为外，很多时候还关心业务的状态（当前状态、历史状态）。以超市零售业务为例，管理人员和分析人员除了关心销售情况，还会关心商品的库存情况看，例如那些商品库存告磬需要补货、那些积压需要促销，而这正是快照事实表（也叫周期快照事实表）所要解决的范畴。
   所谓周期快照事实表，是指间隔一定的周期对业务的状态进行一次拍照并记录下来的事实表。最常见的例子是销售库存、银行账户余额等。

3. **累计快照事实表**

   相对于前两种类型，不是那么常见，但对于某些业务场景来说非常有价值。

   累计快照事实表非常适用于具有工作流或者流水线业务的分析，这些业务通常设计多个事件节点或者有主要的里程碑事件，而累计快照事实表正是从全流程角度对其业务状态的拍照。

4. **无事实的事实表**

   在维度建模中，事实表是过程度量的核心，也是存储度量的地方。但事实表并不总是需要包含度量和事实。这类不包含事实的事实表被称为**无事实的事实表**。

5. **汇总的事实表**

   出于对性能以及下游使用便捷性的考虑，数据仓库还经常对事实表预先进行聚合和汇总。

   当让，如果上游明细有任何改动或者更新，如果需要重新更新汇总的事实表，那么重刷事实表的代价也是比较大的。

### 样例

**商品销售案例**

![数仓相关内容-传统数据模型适用于维表.png](https://z3.ax1x.com/2021/11/15/I2ol7Q.png)

**当前项目案例**

![4Ze0BR.png](https://z3.ax1x.com/2021/09/15/4Ze0BR.png)

## CDC

Change Data Capture，变更数据捕获，下面是来自维基百科的解释。

> In database, change data capture is set of software design patterns used to determine and track the data that has changed so that action can be taken using the changed data. CDC is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.

在数据库中，**变更数据捕获**是一组用于确定和跟踪已更改的数据，以便可以使用已更改的数据采取行动的软件设计模式。 CDC 是一种数据集成方法，它基于对企业数据源所做更改的识别、捕获和分发。 

### Canal

![](https://camo.githubusercontent.com/a9d41336a07581745beaa933f28a8f1860cb4eda/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303139313130343130313733353934372e706e67)


canal [kə'næl]，译意为水道/管道/沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费

早期阿里巴巴因为杭州和美国双机房部署，存在跨机房同步的业务需求，实现方式主要是基于业务 trigger 获取增量变更。从 2010 年开始，业务逐步尝试数据库日志解析获取增量变更进行同步，由此衍生出了大量的数据库增量订阅和消费业务。

基于日志增量订阅和消费的业务包括

1. 数据库镜像
2. 数据库实时备份
3. 索引构建和实时维护(拆分异构索引、倒排索引等)
4. 业务 cache 刷新
5. 带业务逻辑的增量数据处理

当前的 canal 支持源端 MySQL 版本包括 5.1.x , 5.5.x , 5.6.x , 5.7.x , 8.0.x

### MySQL主备复制原理

下面的图来自 Canal Github 官网，但是原图是 “High Performance MySQL”《高性能 MySQL》中的内容。其中包含了 relay log（中继日志），还有对应的 I/O 线程，binlog 内容。具体的实现，是 RingBuffer（环形缓冲区），在《算法》第 4 版中有提到过，其实现比较简单，感兴趣的可以自行搜索。对应的 Redis 的主从同步，也是采用该机制，环形缓冲区是**进程间同步**比较常用的方法。之前提到的 RDD 其实就是用的这个。

这里注意的是，如果你用了 MySQL 主从，开启的是 ROW 格式的 binlog，如果你用 now() 函数就会导致问题。因为同步是需要时间的，这个 now() 就会导致内容不一致的问题。当然，在 Cannal 中不会有这个问题。

![MySQL主从复制.jpg](https://i.loli.net/2021/10/08/xuiwI5QrsqUv4K9.jpg)

- MySQL Master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看)
- MySQL Slave 将 Master 的 binary log events 拷贝到它的中继日志(relay log)
- MySQL Slave 重放 relay log 中事件，将数据变更反映它自己的数据

### Canal 工作原理

- Canal 模拟 MySQL Slave 的交互协议，伪装自己为 MySQL Slave ，向 MySQL Master 发送dump 协议
- MySQL Master 收到 dump 请求，开始推送 binary log 给 Slave (即 Canal)
- Canal 解析 binary log 对象(原始为 byte 流)

## 关系型数据库离线同步流程

![4mC1AS.png](https://z3.ax1x.com/2021/09/16/4mC1AS.png)

# 数据仓库

## 概念

数据仓库是一种 OLAP 数据库，它通过 ETL 从 OLTP 数据库中获得数据，优化整理后创建一个分析平台，根据用户要求提供不同类型的数据集合，用于数据的深度理解与分析<sup>《数据仓库与数据挖掘》</sup>。

数据仓库是一种数据管理系统，旨在启动和支持**商务智能**活动，尤其是做分析。数据仓库仅用于执行查询和分析，通常包含大量历史数据。数据仓库中的数据通常来自广泛的来源，例如应用程序日志文件和事务应用程序。

数据仓库集中并整合来自多个来源的大量数据。它的分析能力使组织能够从他们的数据中获得有价值的业务洞察力，以改进决策。随着时间的推移，它会建立一个对数据科学家和业务分析师来说非常宝贵的历史记录。 由于这些功能，数据仓库可以被视为组织的“单一事实来源”。

数据仓库提供了总体且独特的益处，允许组织分析大量时变数据并从中提取重要价值，以及保留历史记录。

由数据仓库之父的计算机科学家 William Inmon 描述，数据仓库的四个特征：

-   **面向主题的**（**Subject-oriented**）：他们可以分析有关特定主题或功能领域的数据（例如销售）。

-   **集成的**（**Integrated**）：数据仓库在来自不同来源的不同数据类型之间创建一致性。

-   **相对稳定的**（**Nonvolatile**）：一旦数据进入数据仓库，它将稳定不变。

-   **时变的**（**Time-variant**）：数据仓库分析着眼于随时间的变化。

一个设计良好的数据仓库将非常快速地执行查询，提供高吞吐量，并为最终用户提供足够的灵活性来 “切片和切块” 或规约数据卷以进行仔细检查以满足各种需求——无论是在高层次或非常精细、详细的层次。 数据仓库作为中间件 BI 环境的功能基础，为最终用户提供报告、仪表板和其他接口。

一个典型的数仓应该有以下元素：

1. 数据库
2. ETL 解决方案
3. 元数据管理
4. 访问工具——**统计分析、报告和数据挖掘**功能，用于向业务用户可视化和呈现数据的客户端分析工具
5. 数据集市
6. 数据仓库管理工具
7. 信息发布系统

## 数据库

数据库是整个数据仓库环境的核心，是存放数据的地方，提供对数据检索的支持。相对于事务型数据库，其突出特点是对海量数据的支持和快速的检索技术。

## ETL

把数据从各种各样的存储方式中抽取出来，进行必要的转换和整理，再存放到数据仓库内。主要操作包括**删除对决策应用没有意义的数据段**，**转换到统一的数据名称和定义**，**计算统计和衍生数据**，给**缺值数据赋予默认值**，**统一不同的数据格式**等。
![](https://res.weread.qq.com/wrepub/epub_31630789_78)

### Extract

**主要功能**

1. 数据提取
2. 数据清洁
3. 数据转换
4. 衍生数据生成

**抽取方式**

1. 触发器——每当源表中的数据发生变化，就被相应的触发器将变化的数据写入一个临时表，抽取线程从临时表中抽取数据。
2. 时间戳——它是一种基于递增数据比较的增量数据捕获方式，在源表上增加一个时间戳字段，系统中更新修改表数据的时候，同时修改时间戳字段的值。
3. 全表比对——典型方式是采用 MD5 校验码。
4. 日志比对——通过分析数据库自身的日志来判断变化的数据。

**数据清理**

1. 预处理——预先诊断和检测新的数据加载文件，特别是新的文件和数据集。
2. 标准化处理——为名字和地址建立辅助表或联机字典，据此检查和修正名字和地址。
3. 查重——应用各种数据查询手段，避免引入重复数据。
4. 出错处理和修正——将出错的记录和数据写入日志文件，留待进一步处理。

### Transform

包含以下功能

1. 字段转换——主要是指数据类型转换，增加“上下文”数据，例如时间戳；将数值型的地域编码替换成地域名称，如解码（decoding）等。
2. 清洁和净化——主要是保留字段具有特定值或特定范围的记录；引用完整性检查；去除重复记录等。
3. 多数据源整合——主要包括字段映射、代码变换、合并、派生等。
4. 聚合、汇总——事务性数据库侧重于细节，数据仓库侧重于高层次的聚合和汇总

如下图所示

![数据集成.png](https://i.loli.net/2021/05/24/iKnJ1Rtzkbarsmx.png)

### Load

加载有两种方式

1. 刷新方式——采用在定期的间隔对目标数据进行批量重写的技术。目标数据起初被写进数据仓库，然后每隔一定的时间，数据仓库被重写，替换以前的内容。现在这种方式用得比较少。
2. 更新方式——只将源数据中的数据改变写进数据仓库的方法。为了支持数据仓库的周期，便于历史分析，新记录通常被写进数据仓库中，但不覆盖和删除以前的记录，而是通过时间戳来分辨它们。

刷新方式通常用于数据仓库首次被创建时填充数据仓库，更新方式通常用于目标数据仓库的维护。刷新方式通常与全量抽取相结合，而更新方式常与增量抽取相结合。

一般情况下，**第一次将数据加载到数仓时，是全量的，后面都是增量更新**，也就是对应上面的刷新和更新。

## 元数据管理

### 元数据

元数据就是描述数据的数据。

元数据描述数据仓库中的数据，是数据仓库运行和维护的中心。数据仓库服务器利用元数据来存贮和更新数据，用户通过元数据来了解和访问数据。

元数据在数仓中一般分为三类

1. 数据字典
2. 数据血缘
3. 数据特征

**数据字典**

数据字典描述的是数据的结构信息。包括但不限于

- 表名
- 注释信息
- 表的产出任务
- 表的字段名称
- 字段类型及字段含义

**数据血缘**

数据血缘是指一个表是直接通过哪些表加工而来，一般会帮我们做影响分析和故障溯源。

**数据特征**

数据特征主要是指数据的属性信息。包括但不限于

- 表的存储空间大小（如该表 20GB）
- 访问热度（如一天访问该表多少次）
- 主题域
- 分层（例如当前是 dwd 层）
- 表关联的指标

### 根据领域相关性划分

根据领域相关性，可将元数据分为以下四类。

1. 与特定领域相关的元数据，描述数据在特定领域内的公共属性。
2. 与特定领域无关的元数据，描述所有数据的公共属性。
3. 与模型相关的元数据，描述信息和元信息建模过程的数据，又可进一步分为横向模型和纵向模型两类。
4. 其他元数据，例如系统硬件、软件描述和系统配置描述等。

### 根据应用场合划分

根据不同的应用场合，可将元数据分为以下两类。

1. 数据元数据，又称为信息系统元数据，信息系统使用元数据描述信息源，以按照用户需求检索、存取和理解源信息，保证在新的应用环境中使用信息，支持整个信息系统的演进。
2. 过程元数据，又称为软件结构元数据，是关于应用系统的信息，帮助用户查找、评估、存取和管理数据。大型软件结构中包括描述各个组件接口、功能和依赖关系的元数据，这些元数据保证了软件组件的灵活、动态配置。

###  根据内容划分

根据不同内容，可将元数据分为以下四类。

1. 内容（Content），识别、定义、描述基本数据元素，包括数据单元、合法值域等。
2. 结构（Structure），在相关范围内定义数据元素的逻辑概念集合。
3. 表达（Representation），描述每一个值域学（多为技术相关）的物理表示，以及数据元素集合的物理存储结构。
4. 文法（Context），提供基础数据的族系和属性评估，包括所有与基础数据的收集、处理和使用相关的信息。

### 根据用途划分

根据不同的具体用途，可将元数据分为以下两类。

1. 技术元数据（Technical Metadata），存储关于数据仓库系统技术细节的数据，是用于开发和管理数据仓库使用的数据，保证数据仓库系统的正常运行
2. 业务元数据（Business Metadata），从业务角度描述数据仓库中的数据，提供介于使用者和实际系统之间的语义层，帮助数据仓库使用人员理解数据仓库中的数据。

### 元数据管理工具

- Metacat
- DataHub
- Atlas
- Cloudera Navigator

## 访问工具

为用户访问数据仓库提供工具，例如数据查询和报表、应用开发、管理信息系统、OLAP、数据挖掘。

业界常用的 BI 工具

- PowerBI
- Tableau
- FineBI

## 数据集市

数据集市是为了特定的应用目的或应用范围，面向企业的某个部门（或主题），在逻辑或物理上划分出来的数据仓库的子集，也可称为部门数据或主题数据。

在数据仓库的具体的实施过程中，根据主题将数据仓库划分为多个数据集市，从一个部门的数据集市着手，以后再用几个数据集市组成一个完整的数据仓库，有利于数据仓库的负载均衡，保证应用效率。

![数仓相关内容-数据仓库和数据集市的关系](https://z3.ax1x.com/2021/11/15/I2ohHe.png)

## 数据仓库管理工具

安全和特权管理，跟踪数据的更新，检查数据质量，管理和更新元数据，审计和报告数据仓库的使用和状态，删除数据，复制、分割和分发数据，备份和恢复，存储管理。

## 信息发布系统

把数据仓库中的数据或其他相关的数据发送给不同的地点或用户。基于 Web 的信息发布系统是对付多用户访问的有效方法。

例如，对于表的变更，应该有对应的通知上下游人员。

![img](https://static001.geekbang.org/resource/image/b6/98/b6c56a1812116b9961a443ca6e11ab98.jpg)


## OLTP & OLAP

OLTP 和 OLAP 是数据仓库常见名词，下面解释两个的含义。

### OLTP

联机事务处理（On-Line Transactional Processing）<sup>来自 Wikipedia</sup>

> In online transaction processing (OLTP), information systems typically facilitate and manage transaction-oriented applications. The term "transaction" can have two different meanings, both of which might apply: in the realm of computers or database transactions it denotes an atomic change of state, whereas in the realm of business or finance, the term typically denotes an exchange of economic entities (as used by, e.g., Transaction Processing Performance Council or commercial transactions.) OLTP may use transactions of the first type to record transactions of the second.

在联机事务处理 (OLTP) 中，信息系统通常会促进和管理面向事务的应用程序。 术语 “Transaction” 可以有两种不同的含义，这两种含义都可能适用：	

1. 在计算机或数据库交易领域，它表示状态的原子变化，
2. 而在商业或金融领域，该术语通常表示经济交换实体（如事务处理性能委员会或商业事务所使用的那样）。
   OLTP 可以使用第一种类型的事务来记录第二种类型的事务。

### OLAP

联机分析处理（On-Line Analytical Processing，OLAP）<sup>来自 Wikipedia</sup>

> Online analytical processing, or OLAP (/ˈoʊlæp/), is an approach to answer multi-dimensional analytical (MDA) queries swiftly in computing. OLAP is part of the broader category of business intelligence, which also encompasses relational databases, report writing and data mining. Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM), budgeting and forecasting, financial reporting and similar areas, with new applications emerging, such as agriculture.

联机分析处理，又名 OLAP (/ˈoʊlæp/) 是一种在计算中快速响应多维分析 (MDA) 查询的方法。 OLAP 是更广泛的商务智能类别的一部分，该类别还包含关系数据库、分析报告编写和数据挖掘。 OLAP 的典型应用包括销售、市场营销、管理报告、业务流程管理 (BPM)、预算和预测、财务报告和类似领域的业务报告，以及新应用的出现，例如农业。

OLAP 的基本操作含

- 切块
- 切片
- 旋转
- 聚合
- 钻取

OLAP 的分类

- ROLAP
- MOLAP
- HOLAP

#### 基本操作

在实际 OLAP 应用中，OLAP 分析需要对组织好的数据进行各种基本操作，使得用户能够从多个角度、多个侧面观察数据库中的数据，更加深刻地理解数据，发现数据中的有用信息。**切块**是从数据区间上进行取舍，是**切片**的数据准备，可以通过切块得到决策需要的切片。对每一个多维数组，都可以根据决策分析的需要，将一些维定格到一个维成员，使多维数组简化为“切片”或“切块”。多维数据集的“切片”或“切块”数量取决于所选定维的维成员数量。例如，一个酒店经营的数据，若将时间维度选定为“季度”，客户维度选定为“流动客户”，则切片可以揭示该酒店一季度经营中流动客户的消费同促销手段之间的联系。**钻取（包括下钻和上卷）**，下钻是指从概括性的数据出发，深入相应的更详细的数据进行观察或增加新维，使用户在多层数据中获得更多的细节数据；上钻是指从详细的数据中获得相应的概括性的数据，或者减少维数，使用户在多层数据中获得更多的概括性的数据。

![](https://res.weread.qq.com/wrepub/epub_31630789_80)

<table>
	<thead>
		<tr talign="center">
			<th align="center" >
				名称
			</th>
			<th align="center">
				内容
			</th>
			<th align="center">
				目的
			</th>
		</tr>
	</thead>
	<tbody>
    	<tr>
			<td align="center">
				切片
			</td>
			<td align="left">
				选定多维数组的一个二维子集来分析数据。切片是在某个或某些维上选定一个属性成员，而在其他维上选取一定区间的属性成员或全部属性成员来观察数据的一种分析方法。即在多维数组的某两个维上分别选取一定区间或全部的维成员，而在其他维上均选定一个维成员 
			</td>
			<td align="left">
				降低多维数据集的维度，以更好地了解多维数据集。它舍弃了数据的其他观察角度，可以使人们将注意力集中在一个二维子集内重点观察数据
			</td>
		</tr>
        <tr>
			<td align="center">
				切块
			</td>
			<td align="left">
				选定多维数组的一个三维子集来观察数据。即在多维数组的某 3 个维上分别选取一定区间或全部的成员属性，而在其他维上均选定一个成员属性
			</td>
			<td align="left">
				降低多维数组集的维，以更好地了解多维数据集。使人们能将注意力集中在较少地维上进行观察
			</td>
		</tr>
        <tr>
			<td align="center">
				旋转
			</td>
			<td align="left">
				把多维数据集显示的维方向改变为用户要求的方向。旋转是改变一个报告或页面显示的维方向，将多维数据集中的不同维交换显示，即在表格中重新安排维的位置（如行列互换）
			</td>
			<td align="left">
				通过改变维的位置得到不同视角的数据，实现用户直观并多角度地查看数据集中不同维之间的关系
			</td>
		</tr>
        <tr>
			<td align="center">
				聚合
			</td>
			<td align="left">
				通过一个维的概念分层向上攀升或者通过维规约，在数据立方体上进行聚集得到的结果
			</td>
			<td align="left">
				实现高层次的数据汇总，呈现整体与部分的关系
			</td>
		</tr>
        <tr>
			<td align="center">
				钻取
			</td>
			<td align="left">
				聚合的逆操作，指改变维的层次，变换分析的粒度。它包含向下钻取和向上钻取（即上卷）操作，钻取的深度与维所划分的层次相对应
			</td>
			<td align="left">
				为了在不同的综合层次上观察数据
			</td>
		</tr>
	</tobdy>
</table>

#### ROLAP

ROLAP（Relational OLAP，关系联机分析处理）以关系数据库为核心，将分析用的多维数据存储在关系数据库中，并根据应用需要，把应用频率比较高、计算工作量比较大的SQL查询定义为实视图，作为表存储在关系数据库中，即将多维数据映像成平面关系表中的行。ROLAP主要通过一些软件工具或中间软件实现，物理层采用关系数据库的存储结构，又称为虚拟OLAP（Virtual OLAP）。

#### MOLAP

MOLAP（Multidimensional OLAP，多维联机分析处理）将OLAP分析所用到的多维数据在物理上存储为多维数组的形式，形成“立方体”的结构，从而通过多维数组的存储引擎，显示数据的多维视图。

#### HOLAP

HOLAP（Hybrid OLAP，混合联机分析处理）把 MOLAP 和 ROLAP 两种结构的优点结合起来，同时实现了ROLAP 较大的可规模性和MOLAP 的快速计算

### 维（Dimension）

维（dimension）是一种高层次的类型划分，为决策分析时的一类属性，集合构成一个维。

## 逻辑模型

### 再谈事实表和维表

事实表和维表的关系，构成了以下逻辑模型

### 星型

星型模式的每个维度都对应一个唯一的维表，维的层次关系全部通过维表中的字段实现，所有与某个事实有关的维都通过该维度对应的维表直接与事实表关联，所有维表的主键字组合起来作为事实表的主键字。星型模式的维表只与事实表发生关联，维表与维表之间没有任何关系。
![数仓相关内容-星形连接.png](https://z3.ax1x.com/2021/11/15/I2TivV.png)

特点

1. **维表非规范化**
   维表保存了该维度的所有层次信息，减少了查询时数据关联的次数，提高了查询效率。但是维表之间的数据共用性较差。
2. **事实表非规范化**
   所有维表都直接和事实表关联，减少了查询时数据关联的次数，提高了查询效率。但是限制了事实表中关联维表的数量，如果关联的维表数量过多将会造成数据大量冗余，同时对事实表进行索引也很困难。
3. **维表和事实表的关系是一对多或一对一**
   维表中的主键字在事实表中作为外键字存在。如果维表和事实表之间是多对多的关系时，则不能直接采用星型模式，必须对维表或者事实表进行处理，如对维表中的成员组合进行编码或者在事实表中加入新的字段，都要求成员的组合数量固定，但如果数量不固定，同时维表的数据量又很大，星型模式的实现就较为困难。

### 雪花型

星型模式通过主关键字和外关键字把维表和事实表联系在一起。事实上，维表只与事实表关联式规范化的结果。如果将经常合并在一起使用的维度进行规范化，就把星型模式扩展为雪花型模式。

雪花型模式对维表规范化，原有的维表被扩展为小的事实表，用不同维表之间的关联实现维的层次。它把细节数据保留在关系型数据库的事实表中，聚合后的数据也保存在关系型的数据库中，需要更多的处理时间和磁盘空间来执行一些专为多维数据库设计的任务。


特点：

1. 维表的规范化实现了维表重用，简化了维护工作。但是，查询时使用雪花型模式要比星型模式进行更多的关联操作，反而降低了查询效率。
2. 雪花型模式中有些维表并不直接和事实表关联，而是与其他维表关联，特别是派生维和实体属性对应的维，这样就减少了事实表中的一条记录。因此，当维度较多，特别是派生维和实体属性较多时，雪花型模式较为适合。但是，当按派生维和实体属性维进行查询时，首先要进行维表之间的关联，然后再与事实表关联，因此查询效率低于星型模式。
3. 用雪花型模式可以实现维表和事实表之间多对多的关系。

[![4ez2o4.png](https://z3.ax1x.com/2021/09/16/4ez2o4.png)](https://imgtu.com/i/4ez2o4)

### 星型-雪花型

星型-雪花型模式是星型和雪花型模式的结合，在使用星型模式的同时，将其中的一部分维表规范化，提取一些公共的维表。这样打破了星型模式只有一个事实表的限制，且这些事实表共享全部或部分维表，既保证较高的查询效率，又简化维表的维护。

![数仓相关内容-星型-雪花结构.png](https://z3.ax1x.com/2021/11/15/I2TeUJ.png)

## 物理模型

这不是重点，这里不谈，有兴趣的可以看《数据仓库与数据挖掘》这本书上的内容。

## 构建离线数仓中间件/框架

- Hive （不介绍，这个项目不能用）
- Spark（不介绍，这个项目不能用，也有 Spark SQL 桥接 Hive 的）
- Spring Batch + Spring Cloud Data Flow（简单介绍）

### Spring Batch 简单介绍

以下内容来自 Spring Batch 官方文档

> Spring Batch is a lightweight, comprehensive batch framework designed to enable the development of robust batch applications vital for the daily operations of enterprise systems. Spring Batch builds upon the characteristics of the Spring Framework that people have come to expect (productivity, POJO-based development approach, and general ease of use), while making it easy for developers to access and leverage more advance enterprise services when necessary. Spring Batch is not a scheduling framework. There are many good enterprise schedulers (such as Quartz, Tivoli, Control-M, etc.) available in both the commercial and open source spaces. It is intended to work in conjunction with a scheduler, not replace a scheduler.
>
> Spring Batch provides reusable functions that are essential in processing large volumes of records, including logging/tracing, transaction management, job processing statistics, job restart, skip, and resource management. It also provides more advanced technical services and features that enable extremely high-volume and high performance batch jobs through optimization and partitioning techniques. Spring Batch can be used in both simple use cases (such as reading a file into a database or running a stored procedure) as well as complex, high volume use cases (such as moving high volumes of data between databases, transforming it, and so on). High-volume batch jobs can leverage the framework in a highly scalable manner to process significant volumes of information.

Spring Batch 是一个轻量级、全面的批处理框架，旨在支持开发对企业系统日常运营至关重要的强大的批处理应用程序。 Spring Batch 建立在人们期望的 Spring Framework 特性上（生产力、基于 POJO 的开发方法和一般易用性），同时使开发人员在必要时可以轻松访问和利用更先进的企业服务。但是，Spring Batch 不是调度框架。 商业和开源领域都有许多优秀的企业调度程序（例如 Quartz、Tivoli、Control-M 等）。 它旨在与调度程序一起工作，而不是取代调度程序。

Spring Batch 提供了在处理大量记录时必不可少的可重用功能，包括日志记录/跟踪、事务管理、作业处理统计、作业重启、跳过和资源管理。 它还提供更先进的技术服务和功能，通过优化和分区技术实现极高容量和高性能的批处理作业。 Spring Batch 既可以用于简单的用例（例如将文件读入数据库或运行存储过程），也可以用于复杂的大容量用例（例如在数据库之间移动大量数据，对其进行转换等） 在）。 大量批处理作业可以以高度可扩展的方式利用该框架来处理大量信息。

![Figure 2.1: Batch Stereotypes](https://docs.spring.io/spring-batch/docs/4.3.x/reference/html/images/spring-batch-reference-model.png)

——项目展示

# 实时数仓

## 概念

维基百科和百度百科都没有给出明确的定义，这里参考自[这](https://xie.infoq.cn/article/1eaead6117dc646cade11fa39)。

一个实时数仓应该具备以下能力。

- 备实时数据处理能力，并能够根据业务需求提供实时数据的数仓能力，如可以为运营侧提供实时业务变化、实时营销效果数据。
- 数仓中所有数据，从数据采集、加工处理、数据分发都采用实时方式。
- 从数据建设、数据质量、数据血缘、数据治理等都是采用实时方式。

## 三种架构

1. Lambda
2. Kappa
3. 实时 OLAP

### Lambda 架构

来自 Wikipedia

> Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods. This approach to architecture attempts to balance latency, throughput, and fault-tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of map-reduce.

Lambda 架构是一种数据处理架构，旨在通过利用批处理和流处理方法来处理大量数据。 这种架构方法试图通过使用批处理来提供批处理数据的全面和准确的视图，同时使用实时流处理来提供在线数据的视图来平衡延迟、吞吐量和容错性。 两个视图输出可以在呈现之前合并。 Lambda 架构的兴起与大数据的增长、实时分析以及减少 map-reduce 延迟的驱动相关。

![img](https://static001.infoq.cn/resource/image/19/a0/19d9bbc81b3fba7a2c192e5f1da2eda0.png)

### Kappa 架构

Kappa 架构则移除了离线数仓部分，全部使用实时数据生产。这种架构统一了计算引擎，降低了开发成本。

![img](https://static001.infoq.cn/resource/image/28/75/280ea155c0d2fd5815e341011d685875.png)

### 实时 OLAP

随着实时 OLAP 技术的提升，一个新的实时架构被提出，暂时被称为“实时 OLAP 变体”。简单来说，就是将一部分计算压力从流式计算引擎转嫁到实时 OLAP 分析引擎上，以此进行更加灵活的实时数仓计算。

![](D:\数仓\PNG\实时数仓的OLAP架构.webp)



## 构建实时数仓框架

下面简单介绍**网络流控**以及**构建实时数仓的各个框架**

- Storm
- Spark Streaming
- Kafka Streams
- Flink

### 实时数仓框架的反压(Backpressure)机制

**为什么需要网络流控**

![数仓相关内容-网络流控.png](https://z3.ax1x.com/2021/11/15/I279de.png)

在该图中，Producer 的吞吐率是 2MB/s，Consumer 的是 1MB/s，两者之间存在 1MB/s 的速度差。假设两端都有个 Buffer，会面临两种情况：

1. 如果 Receive Buffer 是有界的：5s 后新到达的数据便会被丢弃。
2. 如果 Receive Buffer 是无界的：Receive Buffer 会持续扩张，最终会导致 Consumer 的内存耗尽。

**网络流控的实现：静态限速**

![数仓相关内容-静态限速.png](https://z3.ax1x.com/2021/11/15/I27Azt.png)

传统的做法可以在 Producer 端实现一个类似 Rate Limter 这样的静态限流，将到达 Send Buffer 发送速率降到 1MB/s，这样就能和 Consumer 端的处理速率匹配起来了。但是这个解决方案有两点限制

1. 事先无法预估 Consumer 到底能承受多大的速率。
2. Consumer 的承受能力通常会动态地波动。

**网络流控的实现：动态反馈/自动反压**

![数仓相关内容-动态反馈.png](https://z3.ax1x.com/2021/11/15/I27Zsf.png)

Consumer 能够及时的给 Producer 做一个 Feedback，告知 Producer 能够承受的速率是多少。动态反馈分为两种：

- **正反馈**：接受速率大于发送速率时发生，告知 Producer 可以把发送速率提上来。
- **负反馈**：接受速率小于发送速率时发生，告知 Producer 降低发送速率。

## Storm

Apache Storm 是一款由 Twitter 开源的大规模分布式流计算平台。是第一代流计算平台。随着各种流式计算平台的出现，Storm 也不断尝试着改进和改变。

Storm 集群由两种节点组成：Master 节点和 Worker 节点。

Master 节点运行 Nimbus 进程，用于代码分发、任务分配和状态监控。Worker 节点运行 Supervisor 进程和  Worker 进程，其中 Supervisor 进程负责管理 Worker 进程的整个生命周期，而 Worker 进程创建 Executor 线程，用于执行具体任务（Task）。在 Nimbus 和 Supervisor 之间，还需要通过 Zookeeper 来共享流计算作业状态，协调作业的调度和执行。
![Pasted image 20210910112235.png](https://z3.ax1x.com/2021/11/15/I27udg.png)

在 Storm 中，通过 Topology（拓扑）、Tuple、Stream、Spout 和 Bolt 等概念来描述一个流计算作业。

### Storm 的反压（Backpressure）实现

![apache](https://jobs.one2team.com/wp-content/uploads/2016/04/apache.png)



上图就是 Storm 里实现的反压机制，可以看到 Storm 在每一个 Bolt 都会有一个监测反压的线程（Backpressure Thread），这个线程一但检测到 Bolt 里的接收队列（recv queue）出现了严重阻塞就会把这个情况写到 ZooKeeper 里，ZooKeeper 会一直被 Spout 监听，监听到有反压的情况就会停止发送，通过这样的方式匹配上
下游的发送接收速率。  

由于篇幅限制，这个 Storm 详细介绍跳过，想深入了解的，可以看官网/相关书籍。

## Spark Streaming

Spark Streaming 是建立在 Spark 框架上的实时流计算框架，提供了可扩展、高吞吐和可容错的实时数据流处理功能。Spark Streaming 构建在 Spark 平台上，充分利用了 Spark 的核心处理引擎。Spark Streaming 将接收的实时数据流分成一个个的 RDD，然后由 Spark 引擎对 RDD 做各种处理，其中每个 RDD 实际是一个小的块数据。所以 Spark Streaming 本质上是将流数据分成一段段块数据后，对器进行连续不断的批处理。
![Pasted image 20210910135021.png](https://z3.ax1x.com/2021/11/15/I2j2v9.png)
**这其实就是 Micro Batch 概念的实现**。
Spark Streaming 包含以下核心概念

1. RDD——Spark 引擎的核心概念，代表一个数据的集合，是 Spark 进行数据处理的计算单元。

2. DStream——Spark Streaming 对流的抽象，代表连续数据流。在系统内部，DStream 由一系列的 RDD 构成，每个 RDD 代表一段时间隔内的数据。

3. Transformation——代表 Spark Streaming 对 DStream 的处理逻辑。和下面要讲的 Flink 里面的 DataStream 其实是一个概念，都是“算子（Operator）”，提供了 map、flatMap、filter、reduce、union、join、transform 和 updateStateByKey 等 API。

4. OutputOperations

   >  Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined（来自 Spark Streaming 官网）

Spark Streaming 将 DStream 输出到控制台、数据库或文件系统等外部系统中的操作。目前 DStream 支持的 Output Operation 包括 print、saveAsTextFiles、saveAsObjectFiles、saveAsHadoopFiles 和 foreachRDD。这些操作会触发外部系统访问，所以 DStream 各种转化的执行实际上由这些操作触发。
和下面要讲的 Flink 的 DataStream 输出成 DataStreamSink 是类似的。
	![Pasted image 20210910135641.png](https://z3.ax1x.com/2021/11/15/I2H0HS.png)
	

Spark Streaming 在 1.3 版本后支持 “精准一次”

### Spark Streaming  的反向（Backpressure）

早期的 Spark 不支持反向压力，但从 Spark 1.5 版本开始，Spark Streaming 引入了反向压力功能。默认情况下，Spark Streaming 的反向压力功能是关闭的。当要使用反向压力功能时，需要将 spark.streaming.backpressure.enabled 设置为 True。

整体而言，Spark 的反向压力功能借鉴了工业控制中 PID 控制器的思路。工作原理如下

首先，当 Spark 处理完每批数据时，统计每批数据的处理结束实践、处理时延、等待时延、处理消息数等信息。

然后，Spark 根据统计信息估计处理速度，并将这个估计值通知给数据生产者。

最后，数据生产者根据估计的处理速度，动态调整生产速度，最终使得生产速度与处理速度相匹配。

![数仓相关内容-SparkStreaming反压.png](https://z3.ax1x.com/2021/11/15/I2H2j0.png)

更详细的介绍，可以自行查看官网/相关书籍。

## Kafka Streams

相比于其他流处理平台，Kafka Streams 最大的特色就是它不是一个平台，至少它不是一个具备完整功能（Full-Fledged）的平台，比如其他框架中自带的调度器和资源管理器，就是 Kafka Streams 不提供的。

Kafka 官网明确定义 Kafka Streams 是一个 Java 客户端库（Client Library）。你可以使用这个库来构建高伸缩性、高弹性、高容错性的分布式应用以及微服务。

使用 Kafka Streams API 构建的应用就是一个普通的 Java 应用程序。你可以选择任何熟悉的技术或框架对其进行编译、打包、部署和上线。

它不是个完整的、平台级的框架，很多内容需要你自己手动实现。所以这里不细讲了，稍微提一下。

## Flink

### 概念

> Apache Flink is a framework and distributed processing engine for stateful computations over _unbounded and bounded_ data streams. Flink has been designed to run in _all_ _common_ _cluster environments_, perform computations at _in-memory speed_ and at _any scale_.

Apache Flink 是一个框架和分布式处理引擎，用于在无界和有界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。

下图来自 Flink 官网，大致介绍了 Flink 的流程。

最左侧就是 Source（数据来源），中间就是 Transformation，右边就是 Sink（数据下沉）。

![img](https://flink.apache.org/img/flink-home-graphic.png)

Flink 是一个主从架构的分布式系统。主节点其实是 JobManager，从节点（Worker 节点）叫 TaskManger。可以由 Yarn 调度也可以由 K8s 调度，也可以不用这些独立部署或者内部构成集群。如下图所示
![The processes involved in executing a Flink dataflow](https://nightlies.apache.org/flink/flink-docs-release-1.13/fig/processes.svg)


### JobManager

_JobManager_ 具有许多与协调 Flink 应用程序的分布式执行有关的职责：它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：

- **ResourceManager**

  _ResourceManager_ 负责 Flink 集群中的资源提供、回收、分配 - 它管理 **task slots**，这是 Flink 集群中资源调度的单位（请参考[TaskManagers](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/concepts/flink-architecture/#taskmanagers)）。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。

- **Dispatcher**

  _Dispatcher_ 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。

- **JobMaster**

  _JobMaster_ 负责管理单个[JobGraph](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/concepts/glossary/#logical-graph)的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。

始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 _leader_，其他的则是 _standby_（请参考 [高可用（HA）](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/ha/overview/)）。

### TaskManagers

_TaskManager_（也称为 _worker_）执行作业流的 task，并且缓存和交换数据流。

必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task _slot_。TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子（请参考[Tasks 和算子链](https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/concepts/flink-architecture/#tasks-and-operator-chains)）。
配置信息如下
![flink-1.png](https://z3.ax1x.com/2021/11/15/I2Ho4J.png)
![flink-slot.png](https://z3.ax1x.com/2021/11/15/I2Hbg1.png)

### DataStream

在 Flink 中，DataStream 用来描述数据流。DataStream 在 Flink 中扮演的角色犹如 Spark 中的 RDD。当然，在其中也支持批处理的概念，就是 DataSet，其内部同样是由 DataStream 构成。与 Spark Streaming 的做法是相反的。
Flink 的 Source、Transformation、Sink 均与 DataStream 有关。

- Source：用于描述 Flink 流数据的输入源，输入的流数据表示为 DataStream。Flink 的Source 来源可以是消息中间件、数据库、文件系统或其他资源。
- Transformation：将一个或多个 DataStream 转化为一个新的 DataStream，是 Flink 实施流处理逻辑的地方。提供 map、flatMap、filter、keyBy、reduce、fold、aggregations、window、union、join、split、select、 和 iterate 类型的 Transformation。算子也分有状态和无状态的，例如 filter 就是无状态的。
- Sink：Flink 将 DataStream 输出到外部系统的地方，如写入控制台（下图就是）、数据库、文件系统或消息中间件等。

下面是一个简单的示例

```java
public class FraudDetectionJob {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Transaction> transactions = env.addSource(new TransactionSource())
                .name("transactions");

        DataStream<Alert> alerts = transactions.keyBy(Transaction::getAccountId)
                .process(new FraudDetector())
                .name("fraud-detector");

        alerts.addSink(new AlertSink())
                .name("send-alerts");

        env.execute("Fraud Detection");
    }

}

class FraudDetector extends KeyedProcessFunction<Long, Transaction, Alert> {

    private static final long serialVersionUID = 1L;

    private static final double SMALL_AMOUNT = 1.00;

    private static final double LARGE_AMOUNT = 500.00;

    private static final long ONE_MINUTE = 60 * 1000;


    @Override
    public void processElement(
            Transaction transaction,
            Context context,
            Collector<Alert> collector) throws Exception {
        Alert alert = new Alert();
        alert.setId(transaction.getAccountId());

        collector.collect(alert);
    }

}
```

### Flink 在当前项目的示例代码

下面的代码，就可以将 dwu_ods.ods_ministation_frmu 更改数据，变更 fmu.ba_ministation 对应的数据。

OdsMinistationFrmu

```java
package com.dahua.dwu.server.ods;

import java.io.IOException;

import com.dahua.dwu.server.util.ParameterToolCreator;
import com.dahua.dwu.server.util.SqlFileUtil;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

import static com.dahua.dwu.server.util.SqlFileUtil.CLASSPATH_PROTOCOL;
import static org.apache.flink.table.api.Expressions.$;

/**
 * @author 261224
 * @version 2021/8/25
 * @since 1.0
 */
public class OdsMinistationFrmu {

    public static void main(String[] args) throws IOException {
        StreamExecutionEnvironment streamEnvironment = StreamExecutionEnvironment.createLocalEnvironment();
        StreamTableEnvironment env = StreamTableEnvironment.create(streamEnvironment);
        ParameterTool parameterTool = ParameterToolCreator.getDefaultParameterToolFromArgsAndPath(args);
        String sql = SqlFileUtil.getSql(CLASSPATH_PROTOCOL + "ods_ministation_frmu.sql", parameterTool);
        env.executeSql(sql);
        String sinkTableSql = SqlFileUtil.getSql(CLASSPATH_PROTOCOL + "sink_ministation_frmu.sql", parameterTool);
        env.executeSql(sinkTableSql);
        Table sourceTable = env.from("ods_ministation_frmu");
        Table midTable = sourceTable.select(
                $("platform_code"),
                $("record_code"),
                $("is_delete"),
                $("create_time"),
                $("update_time"),
                $("longitude").as("baidu_lng"),
                $("latitude").as("baidu_lat"),
                $("longitude").as("amap_lng"),
                $("latitude").as("amap_lat"),
                $("longitude").as("tianditu_lng"),
                $("latitude").as("tianditu_lat"),
                $("ministation_sn"),
                $("ministation_name"),
                $("ministation_addr"),
                $("fire_org_code"),
                $("fire_nature"),
                $("fire_level"),
                $("contact_person"),
                $("contact_tel"),
                $("region_code"),
                $("manage_commander_num"),
                $("profession_technology_num"),
                $("fire_fighter_num"),
                $("people_num"),
                $("car_num"),
                $("is_duty_unit"),
                $("rule_rescue_code"),
                $("description"),
                $("master_name"),
                $("master_phone"),
                $("master_assistant_name"),
                $("master_assistant_phone"),
                $("political_commissar_name"),
                $("political_commissar_phone"),
                $("full_path"),
                $("is_third"),
                $("qr_code"),
                $("qr_type")
        );
        midTable.executeInsert("sink_ministation_frmu");
    }

}
```

ExpressionUtil

```java
package com.dahua.dwu.server.util;

import org.apache.flink.api.java.utils.ParameterTool;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * @author 261224
 * @version 2021/9/9
 * @since 1.0
 */
public final class ExpressionUtil {

    private static final Pattern PATTERN = Pattern.compile("(\\$\\{[^}]+})");


    private ExpressionUtil() {
    }

    public static String replaceExpressionLanguage(String str, ParameterTool parameterTool) {
        String result = str;
        // 找到文件 ${} 有几个，分别是什么
        Matcher matcher = PATTERN.matcher(str);
        while (matcher.find()) {
            // ${} 取出里面的数据
            String elStr = matcher.group(1);
            // 找到对应的值
            String prop = elStr.substring(2, elStr.length() - 1);
            // 替换文件 ${} 内容
            String value = parameterTool.get(prop);
            notNull(value, "No such property key like [" + prop + "]");
            result = result.replace(elStr, value);
        }
        return result;
    }

    public static void notNull(Object object, String message) {
        if (object == null) {
            throw new NullPointerException(message);
        }
    }

}
```

ParameterToolCreator

```java
package com.dahua.dwu.server.util;

import org.apache.flink.api.java.utils.ParameterTool;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;

/**
 * @author 261224
 * @version 2021/8/25
 * @since 1.0
 */
public final class ParameterToolCreator {

    private static final String CLASSPATH_PROTOCOL = "classpath:";

    private static final String DEFAULT_PROPERTIES_PATH = CLASSPATH_PROTOCOL + "job.properties";


    private ParameterToolCreator() {
    }

    /**
     * args 优先，properties 里的配置其次
     *
     * @return ParameterTool
     */
    public static ParameterTool getParameterToolFromArgsAndPath(String[] args, String propertiesPath) throws IOException {
        return getParameterTool(propertiesPath).mergeWith(getParameterTool(args));
    }

    public static ParameterTool getDefaultParameterToolFromArgsAndPath(String[] args) throws IOException {
        return getDefaultParameterTool().mergeWith(getParameterTool(args));
    }

    public static ParameterTool getDefaultParameterTool() throws IOException {
        return getParameterTool(DEFAULT_PROPERTIES_PATH);
    }

    public static ParameterTool getParameterTool(String propertiesPath) throws IOException {
        InputStream inputStream;
        if (propertiesPath.startsWith(CLASSPATH_PROTOCOL)) {
            propertiesPath = propertiesPath.substring(CLASSPATH_PROTOCOL.length());
            inputStream = ParameterToolCreator.class.getClassLoader().getResourceAsStream(propertiesPath);
        } else {
            inputStream = new FileInputStream(propertiesPath);
        }
        return ParameterTool.fromPropertiesFile(inputStream);
    }

    public static ParameterTool getParameterTool(String[] args) {
        return ParameterTool.fromArgs(args);
    }

}
```

SqlFileUtil

```java
package com.dahua.dwu.server.util;

import org.apache.flink.api.java.utils.ParameterTool;

import java.io.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import static com.dahua.dwu.server.util.ExpressionUtil.notNull;
import static com.dahua.dwu.server.util.ExpressionUtil.replaceExpressionLanguage;

/**
 * @author 261224
 * @version 2021/8/26
 * @since 1.0
 */
public final class SqlFileUtil {

    public static final String CLASSPATH_PROTOCOL = "classpath:";



    private SqlFileUtil() {
    }

    public static String getSql(String filePath) throws FileNotFoundException {
        return readerToString(creatReader(filePath));
    }

    public static String getSql(String filePath, ParameterTool parameterTool) throws FileNotFoundException {
        return replaceExpressionLanguage(getSql(filePath), parameterTool);
    }

    private static Reader creatReader(String path) throws FileNotFoundException {
        Reader reader;
        if (path.startsWith(CLASSPATH_PROTOCOL)) {
            path = path.substring(CLASSPATH_PROTOCOL.length());
            InputStream in = SqlFileUtil.class.getClassLoader().getResourceAsStream(path);
            notNull(in, path + "can't find");
            reader = new InputStreamReader(in);
        } else {
            reader = new FileReader(path);
        }
        return reader;
    }

    private static String readerToString(Reader reader) {
        StringBuilder result = new StringBuilder();
        try {
            try (BufferedReader br = new BufferedReader(reader)) {
                String currentLine;
                while ((currentLine = br.readLine()) != null) {
                    result.append(currentLine).append("\n");
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result.toString();
    }

}
```

ods_ministation_frmu.sql

```sql
CREATE TABLE IF NOT EXISTS ods_ministation_frmu (
                                      id                        BIGINT,
                                      record_id                 STRING,
                                      platform_code             STRING,
                                      record_code               STRING,
                                      is_delete                 TINYINT,
                                      create_time               TIMESTAMP(3),
                                      update_time               TIMESTAMP(3),
                                      record_update_time        TIMESTAMP(3),
                                      longitude                 DECIMAL(24, 12),
                                      latitude                  DECIMAL(24, 12),
                                      map_type                  TINYINT,
                                      ministation_sn            STRING,
                                      ministation_name          STRING,
                                      ministation_addr          STRING,
                                      fire_org_code             STRING,
                                      fire_nature               STRING,
                                      fire_level                TINYINT,
                                      contact_person            STRING,
                                      contact_tel               STRING,
                                      region_code               STRING,
                                      manage_commander_num      BIGINT,
                                      profession_technology_num BIGINT,
                                      fire_fighter_num          BIGINT,
                                      people_num                BIGINT,
                                      car_num                   BIGINT,
                                      is_duty_unit              TINYINT,
                                      rule_rescue_code          STRING,
                                      description               STRING,
                                      master_name               STRING,
                                      master_phone              STRING,
                                      master_assistant_name     STRING,
                                      master_assistant_phone    STRING,
                                      political_commissar_name  STRING,
                                      political_commissar_phone STRING,
                                      full_path                 STRING,
                                      is_third                  TINYINT,
                                      qr_code                   STRING,
                                      qr_type                   STRING
                ) WITH ( 
                 'connector' = 'kafka', 
                 'topic' = '${topic}',
                 'properties.bootstrap.servers' = '${properties.bootstrap.servers}',
                 'properties.group.id' = '${properties.group.id}',
                 'format' = 'canal-json',
                 'scan.startup.mode' = 'latest-offset'
                )
```

sink_ministation_frmu.sql

```sql
CREATE TABLE sink_ministation_frmu (
                                      platform_code             STRING,
                                      record_code               STRING,
                                      is_delete                 TINYINT,
                                      create_time               DATE,
                                      update_time               DATE,
                                      baidu_lng                 DECIMAL(24, 12),
                                      baidu_lat                 DECIMAL(24, 12),
                                      amap_lng                  DECIMAL(24, 12),
                                      amap_lat                  DECIMAL(24, 12),
                                      tianditu_lng              DECIMAL(24, 12),
                                      tianditu_lat              DECIMAL(24, 12),
                                      ministation_sn            STRING,
                                      ministation_name          STRING,
                                      ministation_addr          STRING,
                                      fire_org_code             STRING,
                                      fire_nature               STRING,
                                      fire_level                TINYINT,
                                      contact_person            STRING,
                                      contact_tel               STRING,
                                      region_code               STRING,
                                      manage_commander_num      BIGINT,
                                      profession_technology_num BIGINT,
                                      fire_fighter_num          BIGINT,
                                      people_num                BIGINT,
                                      car_num                   BIGINT,
                                      is_duty_unit              TINYINT,
                                      rule_rescue_code          STRING,
                                      description               STRING,
                                      master_name               STRING,
                                      master_phone              STRING,
                                      master_assistant_name     STRING,
                                      master_assistant_phone    STRING,
                                      political_commissar_name  STRING,
                                      political_commissar_phone STRING,
                                      full_path                 STRING,
                                      is_third                  TINYINT,
                                      qr_code                   STRING,
                                      qr_type                   STRING,
                                    PRIMARY KEY (record_code) NOT ENFORCED
                            ) WITH (
                                'connector' = 'jdbc',
                                'url' = '${fmu.db-url}',
                                'table-name' = '${fmu.sink-table-name}',
                                'driver'     = 'org.mariadb.jdbc.Driver',
                                'username'   = '${fmu.user-name}',
                                'password'   = '${fmu.password}'
                            )
```

job.properties

```properties
# kafka config
properties.bootstrap.servers=10.35.227.201:9092
topic=dwu_ods_ods_ministation_frmu
properties.group.id=flink
format=json

# ods source config
ods.db-url=jdbc:mysql://10.35.227.203:3306/dwu_ods?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&useSSL=false&zeroDateTimeBehavior=convertToNull&failOverReadOnly=false&treatTinyAsBoolean=false&tinyInt1isBit=false
ods.user-name=DHCloudeL
ods.password=Cloud0#3NZQlbeO
ods.sink-table-name=product

# fmu sink config
fmu.db-url=jdbc:mysql://10.35.227.203:3306/frmu?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&useSSL=false&zeroDateTimeBehavior=convertToNull&failOverReadOnly=false&treatTinyAsBoolean=false&tinyInt1isBit=false
fmu.user-name=DHCloudeL
fmu.password=Cloud0#3NZQlbeO
fmu.sink-table-name=ba_ministation
```

### 流的状态实现

在流数据状态方面，Flink 有关流数据状态的管理都集中在 DataStream 的转化操作上。流按装口分块处理，详情见
Flink 明确的支持三个不同观念的时间

1.  事件发生时间：当事件发生时的时间，作为从设备产生（或存储）事件的记录
2.  入站时间：Flink 在摄取事件时记录的时间戳
3.  处理时间：the time when a specific operator in your pipeline is processing the event

本质上 Flink 是由下面的 Window 实现的，有滚动时间窗口，滑动时间窗口，滚动统计窗口，滑动统计窗口，会话窗口，全局窗口。

![Window assigners](https://ci.apache.org/projects/flink/flink-docs-release-1.13/fig/window-assigners.svg)

[更多术语](https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/concepts/glossary/)

### Flink 的反压（Backpressure）

**Flink 网络传输架构**

这张图就体现了 Flink 在做网络传输的时候基本的数据的流向，发送端在发送网络数据前要经历自己内部的一个流程，会有一个自己的 Network Buffer，在底层用 Netty 去做通信，Netty 这一层又有属于自己 ChannelOutbound Buffer，因为最终是要通过 Socket 做网络请求的发送，所以在 Socket 也有自己的 Send Buffer，同样在接收端也有对应的三级 Buffer。学过计算机网络的时候我们应该了解到，TCP 是自带流量控制的。实际上 Flink （beforeV1.5）就是通过 TCP 的流控机制来实现 feedback 的。

[![4VD9A0.png](https://z3.ax1x.com/2021/09/15/4VD9A0.png)](https://imgtu.com/i/4VD9A0)

**TCP 反压**

这里就不多介绍了，只需要知道 Flink1.5 之前是用的 TCP 流控机制实现的，具体内容见[《Apache Flink 进阶》](https://developer.aliyun.com/topic/download?id=35&utm_content=m_1000290194)该文档。

**Credit-based 反压**

和上面的 TCP 反压类似，由于内容过长，仅需要知道 Flink 1.5 之后实现了自己托管的 credit - based 流控机制，在应用层模拟 TCP 的流控机制。

Flink 在 Sink 的时候，如果外部存储没有反馈给 Sink 端，这种情况下为了防止外部存储在大的数据量下撑爆，可以通过静态限速的方式，在 Source 端做限流。

# 数据湖

## 概念

数据湖定义<sup>来自 Wikipedia</sup>

> A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases, semi-structured data, unstructured data and binary data. A data lake can be established "on premises" or "in the cloud".

数据湖是以自然/原始格式存储的数据的系统或存储库，通常是对象 blob 或文件。 数据湖通常是单一数据存储，包括源系统数据、传感器数据、社交数据等的原始副本，以及用于报告、可视化、高级分析和机器学习等任务的转换数据。 数据湖可以包括来自关系数据库的**结构化数据、半结构化数据、非结构化数据和二进制数据**。 数据湖可以 “在本地” 或 “在云中” 建立。

2010 年，Pentaho 创始人兼 CTO James Dixon 在纽约 Hadoop World 大会上提出了数据湖的概念，他提到：

> 数据湖（Data Lake）是一个以原始格式存储数据的存储库或系统。

下面是数据湖框架 Hudi 的整体架构图

![Hudi Data Lake](http://hudi.apache.org/assets/images/hudi-lake-0fa2b24ade143216839d58ad472df6da.png)

## 我是否需要数据湖？

公司同时使用数据湖和数据仓库来处理来自各种来源的大量数据。选择何时使用其中一种取决于组织打算如何处理数据。下面描述了如何最好地使用它们：

-   **数据湖** 存储大量不同的、未经过滤的数据，稍后用于特定目的。来自业务线应用程序、移动应用程序、社交媒体、物联网设备等的数据在数据湖中被捕获为原始数据。各种数据集的结构、完整性、选择和格式是由进行分析的人在分析时得出的。当组织需要低成本存储来自多个来源的未格式化、非结构化数据并打算在未来用于某些目的时，数据湖可能是正确的选择。

-   **数据仓库** 专门用于分析数据。数据仓库中的分析处理是对已准备好进行分析（收集、上下文和转换）的数据执行的，目的是生成基于分析的见解（insights）。数据仓库还擅长处理来自各种来源的大量数据。当公司需要高级数据分析或分析来利用来自整个企业的多个来源的历史数据时，数据仓库可能是正确的选择。


# 数据中台

## 概念

数据中台就是数据仓库 + 元数据管理工具。

数据中台是企业构建的标准的、安全的、统一的、共享的数据组织，通过数据服务化的方式支撑前端数据应用。

数据中台的核心，是避免数据的重复计算，通过数据服务化，提高数据的共享能力，赋能数据应用。

# 参考书籍/文档

> [《数据仓库与数据挖掘》](https://weread.qq.com/web/reader/ebd3240071e2a5c5ebda250kc7432af0210c74d97b01b1c)

> [《数据仓库》](https://book.douban.com/subject/1881631/)

> [《离线和实时大数据开发实战》](https://book.douban.com/subject/30234022/)

> [《实时流计算系统设计与实现》](https://weread.qq.com/web/reader/1a6324e071c090451a6f854)

> [What Is a Data Warehouse?](https://www.oracle.com/database/what-is-a-data-warehouse/)

> [Flink 官方文档](https://flink.apache.org/)

> [当 TiDB 与 Flink 相结合：高效、易用的实时数仓](https://www.infoq.cn/article/IoD228mbbr7wylDEQKkh#:~:text=Flink%20%2B%20TiDB%20%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%20Flink%20%E6%98%AF%E4%B8%80%E4%B8%AA%E4%BD%8E%E5%BB%B6%E8%BF%9F%E3%80%81%E9%AB%98%E5%90%9E%E5%90%90%E3%80%81%E6%B5%81%E6%89%B9%E7%BB%9F%E4%B8%80%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%EF%BC%8C%E8%A2%AB%E6%99%AE%E9%81%8D%E7%94%A8%E4%BA%8E%E9%AB%98%E5%AE%9E%E6%97%B6%E6%80%A7%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%85%B7%E6%9C%89%E6%94%AF%E6%8C%81%20exactly-once%20%E7%AD%89%E9%87%8D%E8%A6%81%E7%89%B9%E6%80%A7%E3%80%82,TiFlash%20%E4%B9%8B%E5%90%8E%EF%BC%8CTiDB%20%E5%B7%B2%E7%BB%8F%E6%88%90%E4%B8%BA%E4%BA%86%E7%9C%9F%E6%AD%A3%E7%9A%84%20HTAP%EF%BC%88%E5%9C%A8%E7%BA%BF%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%20OLTP%20%2B%20%E5%9C%A8%E7%BA%BF%E5%88%86%E6%9E%90%E5%A4%84%E7%90%86%20OLAP%EF%BC%89%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%82)

> [Flink TiDB 实时数仓](https://www.slidestalk.com/TiDB/FlinkTidbRdw)

> [从 0 开始学大数据](https://time.geekbang.org/column/article/69459)

> [《基于 TiDB 与 Flink 的实时数仓最佳实践白皮书》](https://files.alicdn.com/tpsservice/9e101e568c51c2fbea4690d3c6fd27d0.pdf?spm=a2csy.flink.0.0.49493bdcEgwTag&file=9e101e568c51c2fbea4690d3c6fd27d0.pdf)

> [《Spring Batch 权威指南》](https://book.douban.com/subject/35291955/)

> [《数据密集型应用系统设计》](https://book.douban.com/subject/30329536/)

> [Kafka 核心技术与实战](https://time.geekbang.org/column/intro/191)

> [《Apache Flink 进阶》](https://developer.aliyun.com/topic/download?id=35&utm_content=m_1000290194)

> [聊聊实时数仓架构设计](https://xie.infoq.cn/article/1eaead6117dc646cade11fa39)

> [Canal 官方文档](https://github.com/alibaba/canal)


> [Apache Hudi](http://hudi.apache.org/)

