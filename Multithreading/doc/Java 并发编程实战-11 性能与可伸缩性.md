# 性能与可伸缩性

应用程序的性能可以采用多个指标来衡量，例如服务时间、延迟时间、吞吐率、效率、可伸缩性以及容量等。其中一些指标（例如服务时间、等待时间）用于衡量程序的“运行速度”，即某个指定的任务单元需要“多快”才能处理完成。另一些指标（生产量、吞吐量）用于程序的“处理能力”，即在计算资源一定的情况下，能完成“多少”工作。

**可伸缩性指的是：当增加计算资源时（例如CPU、内存、存储容量或 I/O 带宽），程序的吞吐量或者处理能力能相应地增加**

# Amdahl 定律

在有些问题中，如果可用资源越多，那么问题的解决速度就越快。但是有些任务是串行的，所以一定会影响执行效率。

Amdahl 定律描述是：在增加计算资源的情况下，程序在理论上能够实现最高加速比，这个值取决于程序中可并行组件与串行组件所占的比重，。假定 F 是必须被串行执行的部分，那么根据 Amdahl 定律

Speedup <= 1/(F+(1-F)/N)

例如，串行化组件占 50%，那么最高加速比只能是 2.

# 上下文切换

如果可运行的线程数大于 CPU 的数量，那么操作系统最终会将某个正在运行的线程调度出来，从而使得其他线程能够使用 CPU。这将导致一次上下文切换，在这个过程中将保存当前运行线程的执行上下文，并将新调度进来的线程的执行上下文设置为当前上下文。

**——《Java 并发编程实战》**

CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。

**——《Java 并发编程的艺术》**

在中断周期中，处理器要检查是否有中断发生，如果有中断等待处理，则处理器应进行如下工作：

1. 将当前运行程序的上下文保存。
2. 将程序计数器设为中断处理程序的起始地址。

接着处理器进入取指令周期，开始为中断服务。

所有可能会被中断处理的执行锁改变及重新执行过程所需的信息都必须保存下来。因此，PCB 中的处理器状态信息都是要保存的。

**进程交换 ——《计算机操作系统原理》**

上下文切换的实际开销会随着平台的不同而变化，然而按照经验来看：在大多数通用的处理器中，上下文切换的开销相当于 5 000～10 000 个时钟周期，也就是几微秒。

vmstat 查看上下文切换次数以及在内核中执行时间所占比例等信息。



时钟周期：一个脉冲需要的时间，频率的倒数

CPU周期：读取一个指令节所需的时间

指令周期：读取并执行完一个指令所需的时间

CPU时间片：CPU分给每个进程的时间



在synchronized 和 volatile 提供的可见性保证中可能会使用一些特殊指令，即内存栅栏/屏障（Memory Barrier）。内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。内存栅栏可能同样会对性能带来间接的影响，因为它们将一直一些编译器优化操作。在内存栅栏中，大多数操作都是不能被重排序的。

**锁消除**——编译时做的事情，如果一个锁对象只能由当前线程访问，将会将锁消除（去掉 monitorenter、monitorexit）。

一些更完备的 JVM 能通过逸出/逃逸分析（Escape Analysis）来找出不会发布到堆的本地对象引用（因此这个引用时线程本地的）。说白了就是在栈上创建对象，在 《深入理解 Java 虚拟机》这本书就有说。

**锁粗化（Lock Coarsening）**——将邻近的同步代码块用同一个锁合并起来。

然后提了下 CPU 总线。

通常，总线可分为三类：数据总线，地址总线，控制总线，当然这也适合于CPU总线。在微型机中，CPU作为总线主控，通过控制总线，向各个部件发送控制信号，通过地址总线用地址信号指定其需要访问的部件，如存储器，数据总线上传送数据信息，数据总线是双向的，即，数据信息可由CPU至其它部件（写），也可由其它部件至CPU（读）。CPU总线处于芯片组与CPU之间，负责CPU与外界所有部件的通信，因为CPU是通过芯片组联系各个部件的。此外，CPU总线还负责CPU与Cache之间的通信。正如前面所说，CPU总线像一条主干道，数据和信号从这主干道上流到各个部件和外部设备，也从各个部件流回CPU（主要是数据）。

2.CPU总线的控制和通信　　
CPU总线上的时钟频率通常就是我们常说的外频频率，使用的外频分别有66MHz、100MHz和133MHz三种。而AMD的K7系列CPU虽然也使用100MHz外频，但它的EV6 CPU总线通过DDR（一个时钟脉冲传送两次数据）技术使CPU和芯片组之间的数据传输以200MHz时钟进行，因此EV6总线的数据传输效率最高能达1600MB。

# 阻塞

当在锁上发生竞争时，竞争失败的线程肯定会阻塞。JVM 在实现阻塞行为时，

1. 可以采用自旋等待（Spin-Waiting，指通过循环不断地尝试获取锁，直到成功）。
2. 通过操作系统挂起被阻塞的线程。

如果等待时间短，可以用自旋等待方式，如果等待时间较长，则适合线程挂起方式。有些 JVM 将根据对历史等待时间的分析数据在这两者之间进行选择，但是大多数的 JVM 在等待锁时都只是将线程挂起。

## 减少锁的竞争

> 在并发程序中，对可伸缩性的最主要威胁就是独占方式的资源锁。

两个因素将影响在锁上发生竞争的可能性

1. 锁的请求频率。
2. 每次持有该锁的时间。

两者乘积小，则不会造成影响。

3 种方式可以降低锁的竞争程度：

+ 减少锁的持有时间。
+ 降低锁的请求频率。
+ 使用带有协调机制的独占锁，这些机制允许更高的并发性。

### 减少锁的范围

减少无关的锁的代码。

### 减少锁的粒度

例如 

**DefaultSingletonBeanRegistry#getSingleton**

```java
@Nullable
protected Object getSingleton(String beanName, boolean allowEarlyReference) {
   Object singletonObject = this.singletonObjects.get(beanName);
   if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
      synchronized (this.singletonObjects) {
         singletonObject = this.earlySingletonObjects.get(beanName);
         if (singletonObject == null && allowEarlyReference) {
            ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
            if (singletonFactory != null) {
               singletonObject = singletonFactory.getObject();
               this.earlySingletonObjects.put(beanName, singletonObject);
               this.singletonFactories.remove(beanName);
            }
         }
      }
   }
   return singletonObject;
}
```

### 锁分段

参考 ConcurrentHashMap

`locks[hash % N_LOCKS]`

### 避免热点域

当每个操作都请求多个变量时，锁的粒度将很难降低，这是性能与可伸缩性之间相互制衡的另一个方面，一些常见的优化措施是：

讲一些反复计算的结果缓存起来，都会引入一些热点域（Hot Field），而这些热点域往往会限制可伸缩性。

### 一些替代独占锁的方法

使用并发容器、读 - 写锁、不可变对象以及原子变量。

# 并发程序的测试

## 性能测试

+ 吞吐量——指一组并发任务中已完成任务所占的比例。
+ 响应性——指请求从发出到完成之间的时间。

关于测试的部分，就不详细写了。

