# 总体结构

![pP5W1i9.png](https://z1.ax1x.com/2023/09/20/pP5W1i9.png)

# 自建视频推流服务器

不一定需要推流服务才自建推流服务器，有时可能需要将音频和视频合并流，所以需要本地进行合并。

选择

1. Nginx-rtmp-module（首选）优点是小，功能、文档齐全，背靠 Nginx，稳定。
2. SRS（次选）
3. 其他的不推荐，例如 Red5、Ant Media Server 等

# 推流测试命令

## nginx-rtmp-module 操作手册

https://github.com/arut/nginx-rtmp-module/wiki/Directives#on_connect

```shell
ffmpeg -stream_loop -1 -re -i test.mp4 -c copy -f flv rtmp://localhost/live/stream
```

VCL 验证

```
rtmp://localhost/live/stream
```

## 推流鉴权

如果要限制 ip 地址，则可以在 server 层级下，配置 on_connect 来限制不同 IP 是否能连接，同 on_publish 类似。

推流的参数

```shell
ffmpeg -stream_loop -1 -re -i test.mp4 -c copy -f flv rtmp://localhost/live/1233855?token=1234567895555&username=111&password=111
```

推流的鉴权，在 on_publish 中配置

```nginx
rtmp {
    server {
        listen 1935;
        chunk_size 4000;
        application live {
            live on;
            on_publish http://localhost:8080/auth;
            on_play http://localhost:8080/clientAuth;
        }
    }
}
```

鉴权接口收到的参数

```
{app=live, flashver=FMLE/3.0 (compatible; Lavf60.11, swfurl=, tcurl=rtmp://localhost:1935/live, pageurl=, addr=127.0.0.1, clientid=48, call=publish, name=1233855, type=live, token=1234567895555}
{app=live, flashver=LNX 9,0,124,2, swfurl=, tcurl=rtmp://127.0.0.1:1935/live, pageurl=, addr=127.0.0.1, clientid=50, call=play, name=1233855, start=4294965296, duration=0, reset=0, username=hanyun, password=123456}
```

## 播放鉴权

```shell
rtmp://localhost/live/1233855?username=hanyun&password=123456
```

本地启动 Nginx

[![pP5Rw3q.png](https://z1.ax1x.com/2023/09/20/pP5Rw3q.png)](https://imgse.com/i/pP5Rw3q)

推流

[![pP5R0g0.png](https://z1.ax1x.com/2023/09/20/pP5R0g0.png)](https://imgse.com/i/pP5R0g0)

播放流

[![pP5RruT.png](https://z1.ax1x.com/2023/09/20/pP5RruT.png)](https://imgse.com/i/pP5RruT)

推流鉴权信息

[![pP5RBvV.png](https://z1.ax1x.com/2023/09/20/pP5RBvV.png)](https://imgse.com/i/pP5RBvV)

播放流鉴权信息

[![pP5RdCn.png](https://z1.ax1x.com/2023/09/20/pP5RdCn.png)](https://imgse.com/i/pP5RdCn)

Nginx 配置信息

[![pP5RcE4.png](https://z1.ax1x.com/2023/09/20/pP5RcE4.png)](https://imgse.com/i/pP5RcE4)

# 视频是什么

视频是由图像一帧帧组成的，一秒钟有多少帧图像，就是帧率，视频经过压缩后在网络上的传输 比特率，就是码率。

## 图像

### 像素

像素是图像的基本单元，一个个像素就组成了图像。你可以认为像素就是图像中的一个点。

### 分辨率

图像（或视频）的分辨率是指图像的大小或尺寸。我们一般用像素个数来表示图像的尺寸。比如说一张 1920x1080 的图像，前者 1920 指的是该图像的宽度方向上有 1920 个像素点，而后者 1080 指的是图像的高度方向上有 1080 个像素点。视频行业常见的分辨率有 QCIF（176x144）、CIF（352x288）、D1（704x576 或 720x576），还有我们比较熟悉的 360P（640x360）、720P（1280x720）、1080P（1920x1080）、4K（3840x2160）、8K（7680x4320）等。

### 位深

一般来说，我们看到的彩色图像中，都有三个通道，这三个通道就是 R、G、B 通道。简单来说就是，彩色图像中的像素是有三个颜色值的，分别是红、绿、蓝三个值。

通常 R、G、B 各占 8 个位，也就是一个字节。8 个位能表示 256 种颜色值，那 3 个通道的话就是 256 的 3 次方个颜色值，总共是 1677 万种颜色。我们称这种图像是 8bit 图像，而这个 8bit 就是位深。

我们可以看到，位深越大，我们能够表示的颜色值就越多。因此，图像就可以更精确地展示你拍摄的真实世界。比如现在有 10bit 图像和 12bit 图像，8bit 图像的每一个像素需要占用 3x8 总共 24 个位，3 个字节，同理 10bit、12bit 就会占用更多。

代码样例介绍

java.awt.Color

```java
/**  
 * Creates an sRGB color with the specified red, green, blue, and alpha * values in the range (0 - 255). * * @throws IllegalArgumentException if <code>r</code>, <code>g</code>,  
 *        <code>b</code> or <code>a</code> are outside of the range  
 *        0 to 255, inclusive * @param r the red component  
 * @param g the green component  
 * @param b the blue component  
 * @param a the alpha component  
 * @see #getRed  
 * @see #getGreen  
 * @see #getBlue  
 * @see #getAlpha  
 * @see #getRGB  
 */@ConstructorProperties({"red", "green", "blue", "alpha"})  
public Color(int r, int g, int b, int a) {  
    value = ((a & 0xFF) << 24) |  
            ((r & 0xFF) << 16) |  
            ((g & 0xFF) << 8)  |  
            ((b & 0xFF) << 0);  
    testColorValueRange(r,g,b,a);  
}

```

JS 颜色介绍

16 的平方是 266，所以 JS 的颜色可以如此表示，hex(R)hex(G)hex(B) 共 6 位，样例如下。#00 00 00 表示黑色，#FF FF FF 表示白色

[![pPI8WkQ.png](https://z1.ax1x.com/2023/09/21/pPI8WkQ.png)](https://imgse.com/i/pPI8WkQ)

### Stride

Stride 也可以称之为跨距，是图像存储的时候有的一个概念。它指的是图像存储时内存中每行像素所占用的空间。你可能会问，一张图像的分辨率确定了，那一行的像素值不就确定了吗？为什么还需要跨距这个东西呢？其实，为了能够快速读取一行像素，我们一般会对内存中的图像实现内存对齐，比如 16 字节对齐。

## 视频属性

### 帧率

**帧率**（英语：frame rate）是用于测量显示帧数的[度量](https://zh.wikipedia.org/wiki/%E5%BA%A6%E9%87%8F "度量")。测量单位为“每秒显示帧数”（**f**rame **p**er **s**econd，**FPS**）或“[赫兹](https://zh.wikipedia.org/wiki/%E8%B5%AB%E8%8C%B2 "赫兹")”，

### 码率

视频编码后每秒的数据量称之为码率。

**比特率**（英语：**Bit rate**，[变量](https://zh.wikipedia.org/wiki/%E5%8F%98%E9%87%8F "变量")_R_[[1]](https://zh.wikipedia.org/wiki/%E6%AF%94%E7%89%B9%E7%8E%87#cite_note-1)）在[电信](https://zh.wikipedia.org/wiki/%E7%94%B5%E4%BF%A1 "电信")和[计算](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97_(%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6) "计算 (计算机科学)")领域是指单位时间内传输送或处理的[比特](https://zh.wikipedia.org/wiki/%E4%BD%8D%E5%85%83 "比特")的数量。比特率经常在电信领域用作**连接速度**、**传输速度**、**信息传输速率**和数字[带宽](https://zh.wikipedia.org/wiki/%E5%B8%A6%E5%AE%BD "带宽")容量的同义词。

在数字[多媒体](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%AA%92%E4%BD%93 "多媒体")领域，_比特率_是单位时间播放连续的媒体如压缩后的[音频](https://zh.wikipedia.org/wiki/%E9%9F%B3%E9%A2%91 "音频")或[视频](https://zh.wikipedia.org/wiki/%E8%A7%86%E9%A2%91 "视频")的比特数量。在这个意义上讲，它相当于术语数字**带宽消耗量**，或[吞吐量](https://zh.wikipedia.org/wiki/%E5%90%9E%E5%90%90%E9%87%8F "吞吐量")。

## 颜色空间

### RGB

Red Green Blue

### Y'UV

YUV 图像将亮度信息 Y 与色彩信息 U、V 分离开来。Y 表示亮度，是图像的总体轮廓，称之为 Y 分量。U、V 表示色度，主要描绘图像的色彩等信息，分别称为 U 分量和 V 分量。

Y'代表明亮度(luma; brightness)而U与V存储色度(色讯; chrominance; color)部分; 亮度(luminance)记作Y，而Y'的prime符号记作伽玛校正。

YUV 主要分为 YUV 4:4:4、YUV 4:2:2、YUV 4:2:0 这几种常用的类型。其中最常用的又是 YUV 4:2:0。这三种类型的 YUV 主要的区别就是 U、V 分量像素点的个数和采集方式。

YUV 存储方式主要分为两大类：Planar 和 Packed 两种。Planar 格式的 YUV 是先连续存储所有像素点的 Y，然后接着存储所有像素点的 U，之后再存储所有像素点的 V，也可以是先连续存储所有像素点的 Y，然后接着存储所有像素点的 V，之后再存储所有像素点的 U。Packed 格式的 YUV 是先存储完所有像素的 Y，然后 U、V 连续的交错存储。 

NV21（YUV420SP）

 Packed 格式

4 x 4 像素的 YUV 4:2:0 只需要 24 个字节相比 RGB 图像需要 48 个字节，存储的大小少了一半。也就是说，如果是 8bit 图像，RGB 每一个像素需要 3 个字节。而 YUV 4:2:0 只需要 1.5 个字节。

## 概要介绍



## 视频编码

视频编码重要性

![](https://static001.geekbang.org/resource/image/06/78/064a2356cc7bbf4c1c43d2856e186f78.jpg?wh=1280x572)

现在，我们假设有一个电影视频，分辨率是 1080P，帧率是 25fps，并且时长是 2 小时，如果不做视频压缩的话，它的大小是 1920 x 1080 x 1.5 x 25 x 2 x 3600 = 521.4G。

而对于每一帧图像，又是划分成一个个块来进行编码的，这一个个块在 H264 中叫做宏块，而在 VP9、AV1 中称之为超级块，其实概念是一样的。宏块大小一般是 16x16（H264、VP8），32x32（H265、VP9），64x64（H265、VP9、AV1），128x128（AV1）这几种。这里提到的 H264、H265、VP8、VP9 和 AV1 都是市面上常见的编码标准，

![](https://static001.geekbang.org/resource/image/b9/a9/b94991907d00bd54743cef34b70e01a9.jpg?wh=1085x467)

### H.264

## 帧类型

![](https://static001.geekbang.org/resource/image/6b/8d/6b908464d87e30bf977893ababf7e78d.jpg?wh=1280x392)

### I 帧

帧内编码

特殊帧 

IDR 帧，一般情况用的都是 IDR 帧，以确定以这一帧为基准帧。

### B 帧

RTC 场景不考虑

### P 帧

根据已编码的前一帧进行预测，来推算当前帧大小。



# 音频编码格式

## AAC

## WAV



# RTP


[![pPoVBge.jpg](https://z1.ax1x.com/2023/09/22/pPoVBge.jpg)](https://imgse.com/i/pPoVBge)

RTP（Real-time Transport Protocol）协议，全称是实时传输协议。它主要用于音视频数据的传输。

一般我们在实时通信的时候，需要传输音频和视频数据。我们通常是这样做的，先将原始数据经过编码压缩之后，再将编码码流传输到接收端。在传输的时候我们通常不会直接将编码码流进行传输，而是先将码流打包成一个个 RTP 包再进行发送。那为什么需要打包成 RTP 包呢？这是因为我们的接收端要能够正确地使用这些音视频编码数据，不仅仅需要原始的编码码流，还需要一些额外的信息。

- 当前视频码流标准，是 H.264 还是 H.265 还是 V8
- 播放速度
- 等等

## RTCP 

协议RTCP（Real-time Transport Control Protocol）协议，全称是实时传输控制协议。它是辅助 RTP 协议使用的。RTCP 报文有很多种，分别负责不同的功能。常用的报文有发送端报告（SR）、接收端报告（RR）、RTP 反馈报告（RTPFB）等。而每一种报告的有效载荷都是不同的。我们就是通过这些报告在接收端和发送端传递当前统计的 RTP 包的传输情况的。我们使用这些统计信息来做丢包重传，以及预测带宽。



# FLV 介绍

FLV 全称 FlashVideo

由 H.264和 AAC 及相应协议头部文件构成

FLV 由一个个 Tag 组成，Tag 分为 视频 Tag、音频 Tag 以及 Script Tag，分别用于存储视频、音频以及 MetaData 数据。

![](https://static001.geekbang.org/resource/image/38/c0/38cc18a2a824e2001ae4d38818e691c0.jpeg?wh=1920x1080)

具体格式如下

![](https://static001.geekbang.org/resource/image/d9/00/d9e526be063eb8382535caa585d9f600.jpeg?wh=1552x1190)


FLV Body

其中最重要的是时间戳，因为播放的速度还有音视频同步都需要依赖这个时间戳的值。时间戳占用 3～4 字节，如果 3 字节不够的话，则需要使用 1 字节的扩展时间戳作为时间戳的高 8 位。还需要注意的一个点就是，时间戳的单位是 ms。RTP 的时间戳单位是 1/90000 秒，MP4 的时间戳是可以自定义的。这个时间戳的单位也是至关重要的，不要弄错了。

# FFmpeg

## 1. 介绍


## 2. 使用


## 3. 采用什么语言调用 FFmpeg

对于操作和封装 FFmpeg,Java 和 Python 都有很好的支持,各有优势:

1. Java 有很多成熟的库可以调用 FFmpeg,比如 Xuggler、JavaCV等。这些库提供了面向对象的 API,封装了 FFmpeg 的复杂参数,使用更简单。并且可以利用 Java 的多线程 SUPPORT,进行并行处理。
2. Python 有一些轻量级的封装库,如 ffmpy、ffmpeg-python 等。用 Python 调用 FFmpeg 更简洁,可以将输出解析成 Python 对象,进行后续分析处理。Python 生态中的科学计算库也可以扩展 FFmpeg 的功能。
3. Java 的性能会略高一些,尤其是并行处理很大的媒体文件的时候。但是 Python 更适合快速原型开发与探索性任务。
4. Java 有更完整的生态系统支持企业级应用,Python 更容易嵌入到创新应用与系统中。
5. Java 和 Python 都可以通过编写自定义命令,或对输出进行解析,实现对 FFmpeg 更细粒度的控制。
6. 两者都可以将 FFmpeg 包装成服务,提供 API 或 WEB 接口。

总之,Java 和 Python 对 FFmpeg 的支持都很全面。如果要构建复杂的媒体处理流水线,或者需要最大化性能,Java 更合适。如果要快速编写原型或试验,Python 更易用。熟悉哪种语言就使用哪种语言调用 FFmpeg 可以获得最佳效果。

优先考虑 Python，方便调用 OpenCV 以及下面的 GStreamer.

# GStreamer

适合流
式视频



# 推流设置





# 可参考的案例

- 案例1：[通过python实时生成音视频数据并通过ffmpeg推送和混流](https://zhuanlan.zhihu.com/p/611271483)
- 实战：[超低延时直播技术的落地实践](https://www.infoq.cn/article/r3j9f7ctfjyljwqwwl79)

# 参考内容

1. [攻克视频技术](https://time.geekbang.org/column/article/459554)
2. [搞定音频技术](https://time.geekbang.org/column/article/447740)
3. [《FFmpeg 从入门到精通》](https://weread.qq.com/web/reader/587329805e2f7c587db316ekc81322c012c81e728d9d180)
4. [FFmpeg 官方文档](https://ffmpeg.org/general.html)
5. [OpenCV](https://docs.opencv.org/4.8.0/d0/da7/videoio_overview.html)



相关 Github 资源

https://github.com/0voice/audio_video_streaming/tree/main

https://github.com/xufuji456/FFmpegAndroid

https://github.com/leandromoreira/ffmpeg-libav-tutorial

https://zh.wikipedia.org/zh-cn/Flash_Video

https://github.com/arut/nginx-rtmp-module/wiki/Directives#on_connect